{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Fast, Safe & Simple Asynchronous Task Queues Written In Pure Python Home # Sergeant was written in Insights when Celery failed to work on large scales. At a peak, we had over 100k tasks per second and thousands of workers. Celery was unreliable on that scale. Compared to celery , Sergeant is simpler and easier to understand. This library offers a simple and intuitive interface to implement a queue-based distributed workers architecture instead of supporting a variety of backends and scenarios. Redis and MongoDB are the two supported server backends. Redis is recommended for high throughput environments where performance is paramount. Whenever you need to save your tasks on the disk, you should use MongoDB for stable, consistent systems. There is a third backend called local which is based on SQLite3 database file. We decided not to support AMQP backends in order to keep the library simple. Using some of Python's latest features, such as dataclasses and f-strings , this library is only compatible with Python 3.7 and up. Multiple serializers and compressions are provided by the library. Before pushing a job into the queue, the library serializes it and optionally compresses it. pickle and msgpack are available serializers. The default serializer pickle should not be switched unless it has some limitations, such as security concerns or portability issues. Different serializers support different types of data. Even though Pickle is not portable, it handles more data types than any other serializer. The serialization and deserialization of tasks with parameters not supported by msgpack must be done manually or using Pickle. This library includes multiple built-in compression algorithms that are natively supported by Python. zlib , gzip , bzip2 and lzma . When None is selected, no compression is performed. If you have a lot of tasks queued and need to free up some RAM/storage, using a compressor is recommended. Tasks can be executed using either multiprocessing or threading. Each task may be executed in a different thread or process, depending on the implementor. Concurrency levels can be configured. The library provides two watchdog mechanisms to prevent situations where tasks are running for longer than expected. The first mechanism is a process killer. To ensure that tasks do not exceed the specified maximum timeout, a separate process is created that watches for starting and stopping signals. A thread killer is spawned when using a threading executor. The nature of Python threads means that this killer can only raise an exception in the context of the watched thread. This killer will not work in situations where the thread locks the GIL, such as when executing an infinite regex. Threaded executors should be used with caution. The Supervisor is responsible for managing workers. The supervisor spawns the child workers, each in a different process, and waits for their execution to be completed. Upon completion, the supervisor spawns another child in place of the worker. He also takes care of an out of bounds worker. A worker which violates some optional violations is being killed and replaced with another one. You can specify a memory usage limit.","title":"Home"},{"location":"#home","text":"Sergeant was written in Insights when Celery failed to work on large scales. At a peak, we had over 100k tasks per second and thousands of workers. Celery was unreliable on that scale. Compared to celery , Sergeant is simpler and easier to understand. This library offers a simple and intuitive interface to implement a queue-based distributed workers architecture instead of supporting a variety of backends and scenarios. Redis and MongoDB are the two supported server backends. Redis is recommended for high throughput environments where performance is paramount. Whenever you need to save your tasks on the disk, you should use MongoDB for stable, consistent systems. There is a third backend called local which is based on SQLite3 database file. We decided not to support AMQP backends in order to keep the library simple. Using some of Python's latest features, such as dataclasses and f-strings , this library is only compatible with Python 3.7 and up. Multiple serializers and compressions are provided by the library. Before pushing a job into the queue, the library serializes it and optionally compresses it. pickle and msgpack are available serializers. The default serializer pickle should not be switched unless it has some limitations, such as security concerns or portability issues. Different serializers support different types of data. Even though Pickle is not portable, it handles more data types than any other serializer. The serialization and deserialization of tasks with parameters not supported by msgpack must be done manually or using Pickle. This library includes multiple built-in compression algorithms that are natively supported by Python. zlib , gzip , bzip2 and lzma . When None is selected, no compression is performed. If you have a lot of tasks queued and need to free up some RAM/storage, using a compressor is recommended. Tasks can be executed using either multiprocessing or threading. Each task may be executed in a different thread or process, depending on the implementor. Concurrency levels can be configured. The library provides two watchdog mechanisms to prevent situations where tasks are running for longer than expected. The first mechanism is a process killer. To ensure that tasks do not exceed the specified maximum timeout, a separate process is created that watches for starting and stopping signals. A thread killer is spawned when using a threading executor. The nature of Python threads means that this killer can only raise an exception in the context of the watched thread. This killer will not work in situations where the thread locks the GIL, such as when executing an infinite regex. Threaded executors should be used with caution. The Supervisor is responsible for managing workers. The supervisor spawns the child workers, each in a different process, and waits for their execution to be completed. Upon completion, the supervisor spawns another child in place of the worker. He also takes care of an out of bounds worker. A worker which violates some optional violations is being killed and replaced with another one. You can specify a memory usage limit.","title":"Home"},{"location":"supervisor/","text":"Supervisor # The Supervisor is responsible for spawning and respawning workers. The Supervisor is also responsible for dealing with errors. Supervisors can restrict a worker's memory usage in order to prevent memory exhaustion. concurrent-workers - The number of subprocesses that should be spawned and supervised by the supervisor. worker-module - The worker module in a dotted notation path. worker-class - The class name in the module file, usually Worker . max-worker-memory-usage [optional] - How much RSS memory in bytes a subprocess-worker can utilize before the supervisor terminates it and respawns a new one. logger [optional - programmatically only] - One can supply a custom logger to send all supervisor logs to. Command Line # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 python3 -m sergeant.supervisor --help usage: supervisor.py [ -h ] --concurrent-workers CONCURRENT_WORKERS --worker-class WORKER_CLASS --worker-module WORKER_MODULE [ --max-worker-memory-usage MAX_WORKER_MEMORY_USAGE ] Sergeant Supervisor optional arguments: -h, --help show this help message and exit --concurrent-workers CONCURRENT_WORKERS Number of subprocesses to open --worker-class WORKER_CLASS Class name of the worker to spawn --worker-module WORKER_MODULE Module of the worker class --max-worker-memory-usage MAX_WORKER_MEMORY_USAGE Maximum RSS memory usage in bytes of an individual worker. When a worker reaches this value, the supevisor would kill it and respawn another one in place. Examples # Assuming a Supervisor with concurrency level of 4 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = crawl_worker \\ --worker-class = Worker \\ --concurrent-workers = 4 graph TD Supervisor --> crawl_worker-1 Supervisor --> crawl_worker-2 Supervisor --> crawl_worker-3 Supervisor --> crawl_worker-4 The worker reaches its end of life once it has completed max_tasks_per_run tasks. Supervisor will create a new worker in place. Programatically # It is possilbe to programatically invoke a Supervisor if you would like to document your supervisor parameters or if you would like to attach another logger. Examples # Be sure to pay attention to the worker_module_name parameter. Depending on the command line CWD, Python determines the module name. Look at benchmark/1_simple_worker/sergeant/supervisor.py to see how to work with module names when the 'supervisor.py' file is in the same folder as the worker module. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import sergeant def main (): supervisor = sergeant . supervisor . Supervisor ( worker_module_name = 'consumer' , worker_class_name = 'Worker' , concurrent_workers = 1 , max_worker_memory_usage = None , ) supervisor . start () if __name__ == '__main__' : main ()","title":"Supervisor"},{"location":"supervisor/#supervisor","text":"The Supervisor is responsible for spawning and respawning workers. The Supervisor is also responsible for dealing with errors. Supervisors can restrict a worker's memory usage in order to prevent memory exhaustion. concurrent-workers - The number of subprocesses that should be spawned and supervised by the supervisor. worker-module - The worker module in a dotted notation path. worker-class - The class name in the module file, usually Worker . max-worker-memory-usage [optional] - How much RSS memory in bytes a subprocess-worker can utilize before the supervisor terminates it and respawns a new one. logger [optional - programmatically only] - One can supply a custom logger to send all supervisor logs to.","title":"Supervisor"},{"location":"supervisor/#command-line","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 python3 -m sergeant.supervisor --help usage: supervisor.py [ -h ] --concurrent-workers CONCURRENT_WORKERS --worker-class WORKER_CLASS --worker-module WORKER_MODULE [ --max-worker-memory-usage MAX_WORKER_MEMORY_USAGE ] Sergeant Supervisor optional arguments: -h, --help show this help message and exit --concurrent-workers CONCURRENT_WORKERS Number of subprocesses to open --worker-class WORKER_CLASS Class name of the worker to spawn --worker-module WORKER_MODULE Module of the worker class --max-worker-memory-usage MAX_WORKER_MEMORY_USAGE Maximum RSS memory usage in bytes of an individual worker. When a worker reaches this value, the supevisor would kill it and respawn another one in place.","title":"Command Line"},{"location":"supervisor/#examples","text":"Assuming a Supervisor with concurrency level of 4 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = crawl_worker \\ --worker-class = Worker \\ --concurrent-workers = 4 graph TD Supervisor --> crawl_worker-1 Supervisor --> crawl_worker-2 Supervisor --> crawl_worker-3 Supervisor --> crawl_worker-4 The worker reaches its end of life once it has completed max_tasks_per_run tasks. Supervisor will create a new worker in place.","title":"Examples"},{"location":"supervisor/#programatically","text":"It is possilbe to programatically invoke a Supervisor if you would like to document your supervisor parameters or if you would like to attach another logger.","title":"Programatically"},{"location":"supervisor/#examples_1","text":"Be sure to pay attention to the worker_module_name parameter. Depending on the command line CWD, Python determines the module name. Look at benchmark/1_simple_worker/sergeant/supervisor.py to see how to work with module names when the 'supervisor.py' file is in the same folder as the worker module. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import sergeant def main (): supervisor = sergeant . supervisor . Supervisor ( worker_module_name = 'consumer' , worker_class_name = 'Worker' , concurrent_workers = 1 , max_worker_memory_usage = None , ) supervisor . start () if __name__ == '__main__' : main ()","title":"Examples"},{"location":"examples/base_worker/","text":"Base Worker # In this example, we show how to create a base worker that other workers can inherit from and customize. Code # base.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import sergeant class BaseWorker ( sergeant . worker . Worker , ): def generate_config ( self , ): return sergeant . config . WorkerConfig ( name = 'base_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), ) derived_worker.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import sergeant from . import base class Worker ( base . BaseWorker , ): def generate_config ( self , ): return super () . replace ( name = 'derived_worker' , ) Explanation # To create a base worker class from which workers can be derived, define the base class object's configuration and use the replace method to copy it with different configurations. replace method was implemented by using dataclasses.replace method, to copy the dataclass with different parameters.","title":"Base Worker"},{"location":"examples/base_worker/#base-worker","text":"In this example, we show how to create a base worker that other workers can inherit from and customize.","title":"Base Worker"},{"location":"examples/base_worker/#code","text":"base.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import sergeant class BaseWorker ( sergeant . worker . Worker , ): def generate_config ( self , ): return sergeant . config . WorkerConfig ( name = 'base_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), ) derived_worker.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import sergeant from . import base class Worker ( base . BaseWorker , ): def generate_config ( self , ): return super () . replace ( name = 'derived_worker' , )","title":"Code"},{"location":"examples/base_worker/#explanation","text":"To create a base worker class from which workers can be derived, define the base class object's configuration and use the replace method to copy it with different configurations. replace method was implemented by using dataclasses.replace method, to copy the dataclass with different parameters.","title":"Explanation"},{"location":"examples/single_producer_consumer/","text":"Single Producer-Consumer # We'll start with a simple Consumer-Producer pattern to gain an understanding of how to work with sergeant Graph # graph TD A[Producer] -->|Push Tasks| B{Broker} B -->|Pull Tasks| C[Consumer] C --> B Code # consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import sergeant import logging class Worker ( sergeant . worker . Worker , ): def generate_config ( self , ): return sergeant . config . WorkerConfig ( name = 'test_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), logging = sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ), ) def work ( self , task , ): self . logger . info ( f 'task parameters are: { task . kwargs } ' ) producer.py 1 2 3 4 5 6 7 8 9 10 11 import consumer worker = consumer . Worker () worker . init_broker () worker . purge_tasks () worker . push_task ( kwargs = { 'some_parameter' : 'one' , }, ) Explanation # Consumer # In the class definition, we inherit from sergeant.worker.Worker to gain all the abilities of the Worker class. To make this worker capable of producing and consuming tasks, we must define the generate_config method and the work method. Defining the generate_config class method allows us to configure our worker's abilities. Worker config is a dataclass named sergeant.config.WorkerConfig that has multiple child dataclasses. There are two mandatory fields: name which defines the worker's name and the queue name within the broker, and connector . The connector is responsible for communicating with the broker. The following example shows how to define a worker with a \"redis\" connector. Logging facilitates the configuration of the logger. Each consumed task is handled by the work method. Within the task argument, the parameters are passed as the key kwargs . Producer # The producer loads the consumer module first. The reason is that once a Worker has been instantiated, its configuration can be used. Since the producer uses the Worker instance, it will have a connection to the broker. After that, we call init_broker to create a connection to the Worker instance's task queue. To ensure there are no leftover tasks in the queue, we call purge_tasks . push_task composes a task object, and pushes it to the queue. Execution # Producer 1 python3 producer.py Consumer 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = consumer \\ --worker-class = Worker \\ --concurrent-worker = 1","title":"Single Producer-Consumer"},{"location":"examples/single_producer_consumer/#single-producer-consumer","text":"We'll start with a simple Consumer-Producer pattern to gain an understanding of how to work with sergeant","title":"Single Producer-Consumer"},{"location":"examples/single_producer_consumer/#graph","text":"graph TD A[Producer] -->|Push Tasks| B{Broker} B -->|Pull Tasks| C[Consumer] C --> B","title":"Graph"},{"location":"examples/single_producer_consumer/#code","text":"consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import sergeant import logging class Worker ( sergeant . worker . Worker , ): def generate_config ( self , ): return sergeant . config . WorkerConfig ( name = 'test_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), logging = sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ), ) def work ( self , task , ): self . logger . info ( f 'task parameters are: { task . kwargs } ' ) producer.py 1 2 3 4 5 6 7 8 9 10 11 import consumer worker = consumer . Worker () worker . init_broker () worker . purge_tasks () worker . push_task ( kwargs = { 'some_parameter' : 'one' , }, )","title":"Code"},{"location":"examples/single_producer_consumer/#explanation","text":"","title":"Explanation"},{"location":"examples/single_producer_consumer/#consumer","text":"In the class definition, we inherit from sergeant.worker.Worker to gain all the abilities of the Worker class. To make this worker capable of producing and consuming tasks, we must define the generate_config method and the work method. Defining the generate_config class method allows us to configure our worker's abilities. Worker config is a dataclass named sergeant.config.WorkerConfig that has multiple child dataclasses. There are two mandatory fields: name which defines the worker's name and the queue name within the broker, and connector . The connector is responsible for communicating with the broker. The following example shows how to define a worker with a \"redis\" connector. Logging facilitates the configuration of the logger. Each consumed task is handled by the work method. Within the task argument, the parameters are passed as the key kwargs .","title":"Consumer"},{"location":"examples/single_producer_consumer/#producer","text":"The producer loads the consumer module first. The reason is that once a Worker has been instantiated, its configuration can be used. Since the producer uses the Worker instance, it will have a connection to the broker. After that, we call init_broker to create a connection to the Worker instance's task queue. To ensure there are no leftover tasks in the queue, we call purge_tasks . push_task composes a task object, and pushes it to the queue.","title":"Producer"},{"location":"examples/single_producer_consumer/#execution","text":"Producer 1 python3 producer.py Consumer 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = consumer \\ --worker-class = Worker \\ --concurrent-worker = 1","title":"Execution"},{"location":"examples/supervisor_with_custom_logger/","text":"Supervisor With Custom Logger # This example demonstrates how to create a supervisor module with a custom logger. Code # To work, both files must be in the same directory consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import sergeant class BaseWorker ( sergeant . worker . Worker , ): def generate_config ( self , ): return sergeant . config . WorkerConfig ( name = 'some_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), ) ... supervisor.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import sergeant import logging def main (): parent_package_path = '' if '.' in __loader__ . name : parent_package_path = __loader__ . name . rsplit ( '.' , 1 )[ 0 ] if __loader__ . name == '__main__' : parent_package_path = os . path . dirname ( __loader__ . path ) . replace ( '/' , '.' ) . replace ( ' \\\\ ' , '.' ) logger = logging . getLogger ( name = 'Supervisor' , ) logger . addHandler ( sergeant . logging . logstash . LogstashHandler ( host = 'localhost' , port = 9999 , ), ) logger . setLevel ( level = logging . INFO , ) supervisor = sergeant . supervisor . Supervisor ( worker_module_name = f ' { parent_package_path } .consumer' if parent_package_path else 'consumer' , worker_class_name = 'Worker' , concurrent_workers = 1 , max_worker_memory_usage = None , logger = logger , ) supervisor . start () if __name__ == '__main__' : main ()","title":"Supervisor With Custom Logger"},{"location":"examples/supervisor_with_custom_logger/#supervisor-with-custom-logger","text":"This example demonstrates how to create a supervisor module with a custom logger.","title":"Supervisor With Custom Logger"},{"location":"examples/supervisor_with_custom_logger/#code","text":"To work, both files must be in the same directory consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import sergeant class BaseWorker ( sergeant . worker . Worker , ): def generate_config ( self , ): return sergeant . config . WorkerConfig ( name = 'some_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), ) ... supervisor.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import sergeant import logging def main (): parent_package_path = '' if '.' in __loader__ . name : parent_package_path = __loader__ . name . rsplit ( '.' , 1 )[ 0 ] if __loader__ . name == '__main__' : parent_package_path = os . path . dirname ( __loader__ . path ) . replace ( '/' , '.' ) . replace ( ' \\\\ ' , '.' ) logger = logging . getLogger ( name = 'Supervisor' , ) logger . addHandler ( sergeant . logging . logstash . LogstashHandler ( host = 'localhost' , port = 9999 , ), ) logger . setLevel ( level = logging . INFO , ) supervisor = sergeant . supervisor . Supervisor ( worker_module_name = f ' { parent_package_path } .consumer' if parent_package_path else 'consumer' , worker_class_name = 'Worker' , concurrent_workers = 1 , max_worker_memory_usage = None , logger = logger , ) supervisor . start () if __name__ == '__main__' : main ()","title":"Code"},{"location":"examples/worker_with_apm/","text":"Worker With APM - ElasticAPM # The following example illustrates how to integrate with an APM solution. In this case, it's ElasticAPM . Code # consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 import elasticapm import sergeant import logging import requests class Worker ( sergeant . worker . Worker , ): def generate_config ( self , ): return sergeant . config . WorkerConfig ( name = 'test_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), max_tasks_per_run = 100 , tasks_per_transaction = 1 , max_retries = 3 , logging = sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ), ) def initialize ( self , ): self . apm_client = elasticapm . Client ( server_url = 'http://localhost:8200/' , environment = 'development' , service_name = self . config . name , service_version = '1.0' , auto_log_stacks = True , collect_local_variables = 'errors' , instrument = True , metrics_interval = '30s' , ) def finalize ( self , ): self . apm_client . close () def pre_work ( self , task , ): self . apm_client . begin_transaction ( transaction_type = 'work' , trace_parent = task . trace_id , ) def post_work ( self , task , success , exception , ): if exception is not None : self . apm_client . capture_exception () self . apm_client . end_transaction ( name = 'work' , result = 'success' if success else 'failure' , ) def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url = url_to_crawl , ) response . raise_for_status () producer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from . import consumer def main (): # Init a worker instance to interact with its API worker = consumer . Worker () # Init the worker task queue so we would be able to push tasks to the broker worker . init_broker () # Make sure the queue is empty worker . purge_tasks () # Produce tasks for i in range ( 100 ): worker . push_task ( kwargs = { 'url' : 'https://www.intsights.com/' , }, trace_id = str ( i ) ) if __name__ == '__main__' : main () Explanation # This integration of an APM solution required the implementation of initialize , finalize , pre_work and post_work . initialize - Since this function is called once during the lifetime of the worker, the initialization should take place here. Here we declare the elasticapm.Client . pre_work - This function is called once for every task, before it is executed. This is where we initiate a transaction. post_work - This function runs once per task, after it has been executed. At this point, the transaction is complete. We will also try to capture any exceptions here. finalize - This function will run once during the lifetime of the worker. This is where cleanup will take place. Hence, we will implicitly clean the APM client by calling close . The following example can easily be adapted to any other APM solution, such as jaeger . Execution # Producer 1 python3 producer.py Consumer 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = consumer \\ --worker-class = Worker \\ --concurrent-worker = 1","title":"Worker With APM - ElasticAPM"},{"location":"examples/worker_with_apm/#worker-with-apm-elasticapm","text":"The following example illustrates how to integrate with an APM solution. In this case, it's ElasticAPM .","title":"Worker With APM - ElasticAPM"},{"location":"examples/worker_with_apm/#code","text":"consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 import elasticapm import sergeant import logging import requests class Worker ( sergeant . worker . Worker , ): def generate_config ( self , ): return sergeant . config . WorkerConfig ( name = 'test_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), max_tasks_per_run = 100 , tasks_per_transaction = 1 , max_retries = 3 , logging = sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ), ) def initialize ( self , ): self . apm_client = elasticapm . Client ( server_url = 'http://localhost:8200/' , environment = 'development' , service_name = self . config . name , service_version = '1.0' , auto_log_stacks = True , collect_local_variables = 'errors' , instrument = True , metrics_interval = '30s' , ) def finalize ( self , ): self . apm_client . close () def pre_work ( self , task , ): self . apm_client . begin_transaction ( transaction_type = 'work' , trace_parent = task . trace_id , ) def post_work ( self , task , success , exception , ): if exception is not None : self . apm_client . capture_exception () self . apm_client . end_transaction ( name = 'work' , result = 'success' if success else 'failure' , ) def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url = url_to_crawl , ) response . raise_for_status () producer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from . import consumer def main (): # Init a worker instance to interact with its API worker = consumer . Worker () # Init the worker task queue so we would be able to push tasks to the broker worker . init_broker () # Make sure the queue is empty worker . purge_tasks () # Produce tasks for i in range ( 100 ): worker . push_task ( kwargs = { 'url' : 'https://www.intsights.com/' , }, trace_id = str ( i ) ) if __name__ == '__main__' : main ()","title":"Code"},{"location":"examples/worker_with_apm/#explanation","text":"This integration of an APM solution required the implementation of initialize , finalize , pre_work and post_work . initialize - Since this function is called once during the lifetime of the worker, the initialization should take place here. Here we declare the elasticapm.Client . pre_work - This function is called once for every task, before it is executed. This is where we initiate a transaction. post_work - This function runs once per task, after it has been executed. At this point, the transaction is complete. We will also try to capture any exceptions here. finalize - This function will run once during the lifetime of the worker. This is where cleanup will take place. Hence, we will implicitly clean the APM client by calling close . The following example can easily be adapted to any other APM solution, such as jaeger .","title":"Explanation"},{"location":"examples/worker_with_apm/#execution","text":"Producer 1 python3 producer.py Consumer 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = consumer \\ --worker-class = Worker \\ --concurrent-worker = 1","title":"Execution"},{"location":"worker/config/connector/","text":"connector # The connector parameter is used to configure the broker's connection. However, it is crucial to note that the broker cannot guarantee tasks' order. You can queue two consecutive tasks and consume them in a different order. For applications that do care about order, use only one instance of a broker, such as Redis or MongoDB , and the order would remain constant. Definition # 1 2 3 4 5 6 @dataclasses . dataclass ( frozen = True , ) class Connector : params : typing . Dict [ str , typing . Any ] type : typing . Literal [ 'redis' , 'mongo' , 'local' ] = 'local' The type parameter specifies the type of connector. The library supports the following connector types: redis - A single or multiple redis instances that are not cluster-connected. Client side distribution of tasks is handled by the library by shuffling the list of connections and pushing/pulling from each one at various times. The order of tasks is not guaranteed. mongo - Single/Multiple MongoDB instances that are not clustered. Each server must be configured as a replica set. The library will create the replica set. It's ideally suited for persistent tasks. local - Local is a SQLite3-based connector. It requires a file system to store the database files. With this connector, you can run Sergeant without having to rely on servers. Connectors receive the params parameter directly as **kwargs . Examples # redis-single 1 2 3 4 5 6 7 8 9 10 11 12 13 sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ) redis-multi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, { 'host' : 'localhost' , 'port' : 6380 , 'password' : None , 'database' : 0 , }, ], }, ) mongo-single 1 2 3 4 5 6 7 8 9 10 11 12 sergeant . config . Connector ( type = 'mongo' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 27017 , 'replica_set' : 'replica_set_name' , }, ], }, ) mongo-multi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 sergeant . config . Connector ( type = 'mongo' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 27017 , 'replica_set' : 'replica_set_name' , }, { 'host' : 'localhost' , 'port' : 27018 , 'replica_set' : 'replica_set_name' , }, ], }, ) local 1 2 3 4 5 6 sergeant . config . Connector ( type = 'local' , params = { 'file_path' : '/tmp/sergeant_db.sqlite3' , }, )","title":"connector"},{"location":"worker/config/connector/#connector","text":"The connector parameter is used to configure the broker's connection. However, it is crucial to note that the broker cannot guarantee tasks' order. You can queue two consecutive tasks and consume them in a different order. For applications that do care about order, use only one instance of a broker, such as Redis or MongoDB , and the order would remain constant.","title":"connector"},{"location":"worker/config/connector/#definition","text":"1 2 3 4 5 6 @dataclasses . dataclass ( frozen = True , ) class Connector : params : typing . Dict [ str , typing . Any ] type : typing . Literal [ 'redis' , 'mongo' , 'local' ] = 'local' The type parameter specifies the type of connector. The library supports the following connector types: redis - A single or multiple redis instances that are not cluster-connected. Client side distribution of tasks is handled by the library by shuffling the list of connections and pushing/pulling from each one at various times. The order of tasks is not guaranteed. mongo - Single/Multiple MongoDB instances that are not clustered. Each server must be configured as a replica set. The library will create the replica set. It's ideally suited for persistent tasks. local - Local is a SQLite3-based connector. It requires a file system to store the database files. With this connector, you can run Sergeant without having to rely on servers. Connectors receive the params parameter directly as **kwargs .","title":"Definition"},{"location":"worker/config/connector/#examples","text":"redis-single 1 2 3 4 5 6 7 8 9 10 11 12 13 sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ) redis-multi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, { 'host' : 'localhost' , 'port' : 6380 , 'password' : None , 'database' : 0 , }, ], }, ) mongo-single 1 2 3 4 5 6 7 8 9 10 11 12 sergeant . config . Connector ( type = 'mongo' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 27017 , 'replica_set' : 'replica_set_name' , }, ], }, ) mongo-multi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 sergeant . config . Connector ( type = 'mongo' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 27017 , 'replica_set' : 'replica_set_name' , }, { 'host' : 'localhost' , 'port' : 27018 , 'replica_set' : 'replica_set_name' , }, ], }, ) local 1 2 3 4 5 6 sergeant . config . Connector ( type = 'local' , params = { 'file_path' : '/tmp/sergeant_db.sqlite3' , }, )","title":"Examples"},{"location":"worker/config/encoder/","text":"encoder # encoder specifies how tasks will be serialized and compressed. Definition # 1 2 3 4 5 6 @dataclasses . dataclass ( frozen = True , ) class Encoder : compressor : typing . Optional [ str ] = None serializer : str = 'pickle' The compressor parameter specifies the type of compressor. Each task is compressed before being pushed to the queue. Brokers can reduce storage/memory usage by using compressor . Tasks with a lot of parameters or data can quickly take up a lot of memory. Due to the compression algorithm being a CPU intensive operation, using a compressor would negatively impact task pushing/pulling. You can choose from the following compressors: None [default] - No compression is applied bzip2 gzip lzma zlib The serializer type is defined by the serializer parameter. Whenever a task is pushed to the queue, it should be serialized so the broker can save it as a byte array. Because each serialization algorithm has some limitations, it is critical to choose the right serialization algorithm. These serializers are available: pickle [default]: Pros: fast, native, supports many data types. Cons: insecure (allows arbitrary code to run), non-portable, might deceive into thinking parameters are serialized correctly but finding a broken object upon deserialization. msgpack Pros: fast, portable, and secure. Cons: fewer data types are supported. Any combination of compressor and serializer can be made to suit your needs. Examples # default 1 2 3 4 sergeant . config . Encoder ( compressor = None , serializer = 'pickle' , ) zlib-pickle 1 2 3 4 sergeant . config . Encoder ( compressor = 'zlib' , serializer = 'pickle' , ) lzma-msgpack 1 2 3 4 sergeant . config . Encoder ( compressor = 'lzma' , serializer = 'msgpack' , )","title":"encoder"},{"location":"worker/config/encoder/#encoder","text":"encoder specifies how tasks will be serialized and compressed.","title":"encoder"},{"location":"worker/config/encoder/#definition","text":"1 2 3 4 5 6 @dataclasses . dataclass ( frozen = True , ) class Encoder : compressor : typing . Optional [ str ] = None serializer : str = 'pickle' The compressor parameter specifies the type of compressor. Each task is compressed before being pushed to the queue. Brokers can reduce storage/memory usage by using compressor . Tasks with a lot of parameters or data can quickly take up a lot of memory. Due to the compression algorithm being a CPU intensive operation, using a compressor would negatively impact task pushing/pulling. You can choose from the following compressors: None [default] - No compression is applied bzip2 gzip lzma zlib The serializer type is defined by the serializer parameter. Whenever a task is pushed to the queue, it should be serialized so the broker can save it as a byte array. Because each serialization algorithm has some limitations, it is critical to choose the right serialization algorithm. These serializers are available: pickle [default]: Pros: fast, native, supports many data types. Cons: insecure (allows arbitrary code to run), non-portable, might deceive into thinking parameters are serialized correctly but finding a broken object upon deserialization. msgpack Pros: fast, portable, and secure. Cons: fewer data types are supported. Any combination of compressor and serializer can be made to suit your needs.","title":"Definition"},{"location":"worker/config/encoder/#examples","text":"default 1 2 3 4 sergeant . config . Encoder ( compressor = None , serializer = 'pickle' , ) zlib-pickle 1 2 3 4 sergeant . config . Encoder ( compressor = 'zlib' , serializer = 'pickle' , ) lzma-msgpack 1 2 3 4 sergeant . config . Encoder ( compressor = 'lzma' , serializer = 'msgpack' , )","title":"Examples"},{"location":"worker/config/logging/","text":"logging # The logging parameter controls the worker's logger. Definition # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @dataclasses . dataclass ( frozen = True , ) class LoggingEvents : on_success : bool = False on_failure : bool = True on_timeout : bool = True on_retry : bool = True on_max_retries : bool = True on_requeue : bool = True @dataclasses . dataclass ( frozen = True , ) class Logging : level : int = logging . ERROR log_to_stdout : bool = False events : LoggingEvents = dataclasses . field ( default_factory = LoggingEvents , ) handlers : typing . List [ logging . Handler ] = dataclasses . field ( default_factory = list , ) There are the following configuration options: level [logging.ERROR] - The logger's logging.level . Available levels can be found in the logging library. log_to_stdout [False] - Whether the logger should write to stdout. Events - Events which the logger should record. on_success [False] - When a task is successfully completed. on_failure - Whenever a task fails. on_timeout [True] - When a task timed out. on_retry [True] - Whenever a task asks for retry. on_max_retries [True] - Each time a task asks for a retry beyond the maximum number of tries. on_requeue [True] - Every time a task requeues. handlers - List of handlers [logging.Handler] attached to the logging.Logger object. Examples # STDOUT 1 2 3 4 sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ) Logstash 1 2 3 4 5 6 7 8 9 10 sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , handlers = [ sergeant . logging . logstash . LogstashHandler ( host = 'localhost' , port = 9999 , ), ], )","title":"logging"},{"location":"worker/config/logging/#logging","text":"The logging parameter controls the worker's logger.","title":"logging"},{"location":"worker/config/logging/#definition","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @dataclasses . dataclass ( frozen = True , ) class LoggingEvents : on_success : bool = False on_failure : bool = True on_timeout : bool = True on_retry : bool = True on_max_retries : bool = True on_requeue : bool = True @dataclasses . dataclass ( frozen = True , ) class Logging : level : int = logging . ERROR log_to_stdout : bool = False events : LoggingEvents = dataclasses . field ( default_factory = LoggingEvents , ) handlers : typing . List [ logging . Handler ] = dataclasses . field ( default_factory = list , ) There are the following configuration options: level [logging.ERROR] - The logger's logging.level . Available levels can be found in the logging library. log_to_stdout [False] - Whether the logger should write to stdout. Events - Events which the logger should record. on_success [False] - When a task is successfully completed. on_failure - Whenever a task fails. on_timeout [True] - When a task timed out. on_retry [True] - Whenever a task asks for retry. on_max_retries [True] - Each time a task asks for a retry beyond the maximum number of tries. on_requeue [True] - Every time a task requeues. handlers - List of handlers [logging.Handler] attached to the logging.Logger object.","title":"Definition"},{"location":"worker/config/logging/#examples","text":"STDOUT 1 2 3 4 sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ) Logstash 1 2 3 4 5 6 7 8 9 10 sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , handlers = [ sergeant . logging . logstash . LogstashHandler ( host = 'localhost' , port = 9999 , ), ], )","title":"Examples"},{"location":"worker/config/max_retries/","text":"max_retries # The maximum number of retries a worker can make to complete the task before it will never be called again is specified by the max_retries parameter. It should be a positive integer greater than zero. For tasks that may require retrying, this parameter should be used. When you call retry, the task is returned to the queue and its internal run_count is increased by one. Calling retry again after the run_count reaches the max_retries number will result in an exception. Tasks that should stop after a certain number of retries should define this number. A value of 0 means there can be an infinite number of retries. Definition # 1 max_retries : int = 0","title":"max_retries"},{"location":"worker/config/max_retries/#max_retries","text":"The maximum number of retries a worker can make to complete the task before it will never be called again is specified by the max_retries parameter. It should be a positive integer greater than zero. For tasks that may require retrying, this parameter should be used. When you call retry, the task is returned to the queue and its internal run_count is increased by one. Calling retry again after the run_count reaches the max_retries number will result in an exception. Tasks that should stop after a certain number of retries should define this number. A value of 0 means there can be an infinite number of retries.","title":"max_retries"},{"location":"worker/config/max_retries/#definition","text":"1 max_retries : int = 0","title":"Definition"},{"location":"worker/config/max_tasks_per_run/","text":"max_tasks_per_run # max_tasks_per_run specifies the maximum number of tasks the worker should consume before spawning another worker. This must be a positive integer. Even if you believe your worker should never respawn, you are encouraged to use this parameter. Memory leaks may occur from time to time without any apparent symptoms. Declaring this parameter might help maintain the health of your workers. Low numbers are discouraged unless necessary. Workers who die frequently might feel the overhead of respawning caused by a low number. When the value is 0, the worker will never respawn unless it explicitly requests it. Definition # 1 max_tasks_per_run : int = 0","title":"max_tasks_per_run"},{"location":"worker/config/max_tasks_per_run/#max_tasks_per_run","text":"max_tasks_per_run specifies the maximum number of tasks the worker should consume before spawning another worker. This must be a positive integer. Even if you believe your worker should never respawn, you are encouraged to use this parameter. Memory leaks may occur from time to time without any apparent symptoms. Declaring this parameter might help maintain the health of your workers. Low numbers are discouraged unless necessary. Workers who die frequently might feel the overhead of respawning caused by a low number. When the value is 0, the worker will never respawn unless it explicitly requests it.","title":"max_tasks_per_run"},{"location":"worker/config/max_tasks_per_run/#definition","text":"1 max_tasks_per_run : int = 0","title":"Definition"},{"location":"worker/config/name/","text":"name # A crucial mandatory parameter in the architecture is the name parameter. The worker's ability to consume tasks is dependent on this parameter. The queue name is determined by this parameter. In this case, using the same worker name with the same broker would result in both workers sharing the same queue and eventually consuming each other's tasks. Definition # 1 name : str","title":"name"},{"location":"worker/config/name/#name","text":"A crucial mandatory parameter in the architecture is the name parameter. The worker's ability to consume tasks is dependent on this parameter. The queue name is determined by this parameter. In this case, using the same worker name with the same broker would result in both workers sharing the same queue and eventually consuming each other's tasks.","title":"name"},{"location":"worker/config/name/#definition","text":"1 name : str","title":"Definition"},{"location":"worker/config/number_of_threads/","text":"number_of_threads # This parameter controls how many threads will be used to execute the tasks. Under the hood, it determines which executor should be used to execute tasks. Definition # 1 number_of_threads : int = 1 During task execution, the number_of_threads parameter specifies the number of threads to invoke. When the number of threads is 1, the serial executor is used; otherwise, the threaded executor is used. serial [default] - serial executor takes each of the tasks that were pulled from the broker one at a time and executes each one individually. With this executor, there is no parallelism within the process. threaded - threaded executor pulls a bulk of tasks from the broker, and runs them within a thread pool. With this executor there are multiple threads that execute different tasks simultaneously. Choosing the right executor type is significant. The consequences of choosing the incorrect executor type are more severe than one might imagine. serial executor has a low overhead and is stable. The problem with it is that it won't use system resources if the workload is heavily IO oriented. However, due to its nature, this executor is quite stable. Tasks are serialized and run sequentially. ProcessKiller watches the worker and decides what to do when a problem occurs with one of the tasks. It may kill the Worker's process to prevent it from being stuck indefinitely. threaded executor on the other hand, has much more technical difficulties that should be addressed. On the one hand, it is fast when the tasks are IO bounded. The problem arises when edge cases occur. When a worker encounters a timeout situation while the task is running longer than expected, it is not trivial to signal it. The ThreadKiller uses a Python technique that is not stable and attempts to raise an exception inside the stuck thread to stop it. By Python's nature and how the GIL works, there is no guarantee that the exception will be raised. A worker may become stuck indefinitely. Thus, you should use a threaded executor only when there is no chance the task will become stuck in a GIL locked function.","title":"number_of_threads"},{"location":"worker/config/number_of_threads/#number_of_threads","text":"This parameter controls how many threads will be used to execute the tasks. Under the hood, it determines which executor should be used to execute tasks.","title":"number_of_threads"},{"location":"worker/config/number_of_threads/#definition","text":"1 number_of_threads : int = 1 During task execution, the number_of_threads parameter specifies the number of threads to invoke. When the number of threads is 1, the serial executor is used; otherwise, the threaded executor is used. serial [default] - serial executor takes each of the tasks that were pulled from the broker one at a time and executes each one individually. With this executor, there is no parallelism within the process. threaded - threaded executor pulls a bulk of tasks from the broker, and runs them within a thread pool. With this executor there are multiple threads that execute different tasks simultaneously. Choosing the right executor type is significant. The consequences of choosing the incorrect executor type are more severe than one might imagine. serial executor has a low overhead and is stable. The problem with it is that it won't use system resources if the workload is heavily IO oriented. However, due to its nature, this executor is quite stable. Tasks are serialized and run sequentially. ProcessKiller watches the worker and decides what to do when a problem occurs with one of the tasks. It may kill the Worker's process to prevent it from being stuck indefinitely. threaded executor on the other hand, has much more technical difficulties that should be addressed. On the one hand, it is fast when the tasks are IO bounded. The problem arises when edge cases occur. When a worker encounters a timeout situation while the task is running longer than expected, it is not trivial to signal it. The ThreadKiller uses a Python technique that is not stable and attempts to raise an exception inside the stuck thread to stop it. By Python's nature and how the GIL works, there is no guarantee that the exception will be raised. A worker may become stuck indefinitely. Thus, you should use a threaded executor only when there is no chance the task will become stuck in a GIL locked function.","title":"Definition"},{"location":"worker/config/starvation/","text":"starvation # The starvation logic of the worker is controlled by the starvation parameter. When a worker tries to pull tasks from the queue but fails, he or she is considered to be starving. It can happen in a number of circumstances. First of all, the task queue may be empty. Second, it can occur when other workers pull tasks from the queue faster than the current worker. Third, the worker might be experiencing connectivity problems. There are probably more cases of this. Starvation is the concept of running a worker when it can do nothing related to its purpose. Definition # 1 2 3 4 5 @dataclasses . dataclass ( frozen = True , ) class Starvation : time_with_no_tasks : int The following configurations are available: time_with_no_tasks - How many seconds will be considered a starvation without being able to pull tasks from the queue.","title":"starvation"},{"location":"worker/config/starvation/#starvation","text":"The starvation logic of the worker is controlled by the starvation parameter. When a worker tries to pull tasks from the queue but fails, he or she is considered to be starving. It can happen in a number of circumstances. First of all, the task queue may be empty. Second, it can occur when other workers pull tasks from the queue faster than the current worker. Third, the worker might be experiencing connectivity problems. There are probably more cases of this. Starvation is the concept of running a worker when it can do nothing related to its purpose.","title":"starvation"},{"location":"worker/config/starvation/#definition","text":"1 2 3 4 5 @dataclasses . dataclass ( frozen = True , ) class Starvation : time_with_no_tasks : int The following configurations are available: time_with_no_tasks - How many seconds will be considered a starvation without being able to pull tasks from the queue.","title":"Definition"},{"location":"worker/config/tasks_per_transaction/","text":"tasks_per_transaction # tasks_per_transaction parameter controls how many tasks will be pulled on each transaction against the broker. When your tasks are completed quickly, you might want to change the default number. Pick a number based on the following assumptions: - The larger the number, the less friction with the broker and the lower the overhead for the broker - The greater the number, the more tasks are in the Worker's inner queue that must be completed before the Worker is allowed to communicate with the broker again. - The greater the number, the more workers might be unable to pull tasks from the queue as it becomes empty. - The lower the number, the heavier the load on the broker. - As the number decreases, the task distribution becomes more uniform. Definition # 1 tasks_per_transaction : int = 1","title":"tasks_per_transaction"},{"location":"worker/config/tasks_per_transaction/#tasks_per_transaction","text":"tasks_per_transaction parameter controls how many tasks will be pulled on each transaction against the broker. When your tasks are completed quickly, you might want to change the default number. Pick a number based on the following assumptions: - The larger the number, the less friction with the broker and the lower the overhead for the broker - The greater the number, the more tasks are in the Worker's inner queue that must be completed before the Worker is allowed to communicate with the broker again. - The greater the number, the more workers might be unable to pull tasks from the queue as it becomes empty. - The lower the number, the heavier the load on the broker. - As the number decreases, the task distribution becomes more uniform.","title":"tasks_per_transaction"},{"location":"worker/config/tasks_per_transaction/#definition","text":"1 tasks_per_transaction : int = 1","title":"Definition"},{"location":"worker/config/timeouts/","text":"timeouts # The timeouts parameter controls the killer timeouts for the worker. Definition # 1 2 3 4 5 6 @dataclasses . dataclass ( frozen = True , ) class Timeouts : timeout : float = 0.0 grace_period : float = 10.0 The timeouts parameter controls the timeouts mechanism of the worker. timeout - The number of seconds after which the worker will stop running a specific task. grace_period [serial] - When the worker has been signaled to stop, how many seconds will pass before being killed aggressively. Timeouts are not applied by default. This means that tasks will never time out. Timeouts should be used wisely and according to the expected task type. If the task should run for no more than 30s, you can set the timeout to 1m to prevent it from being stuck forever. Examples # 1 2 3 sergeant . config . Timeouts ( timeout = 10.0 , )","title":"timeouts"},{"location":"worker/config/timeouts/#timeouts","text":"The timeouts parameter controls the killer timeouts for the worker.","title":"timeouts"},{"location":"worker/config/timeouts/#definition","text":"1 2 3 4 5 6 @dataclasses . dataclass ( frozen = True , ) class Timeouts : timeout : float = 0.0 grace_period : float = 10.0 The timeouts parameter controls the timeouts mechanism of the worker. timeout - The number of seconds after which the worker will stop running a specific task. grace_period [serial] - When the worker has been signaled to stop, how many seconds will pass before being killed aggressively. Timeouts are not applied by default. This means that tasks will never time out. Timeouts should be used wisely and according to the expected task type. If the task should run for no more than 30s, you can set the timeout to 1m to prevent it from being stuck forever.","title":"Definition"},{"location":"worker/config/timeouts/#examples","text":"1 2 3 sergeant . config . Timeouts ( timeout = 10.0 , )","title":"Examples"},{"location":"worker/handlers/on_failure/","text":"on_failure # The on_failure handler is called when a task raises an exception. The exception object is passed to the handler. Definition # 1 2 3 4 5 def on_failure ( self , task : sergeant . objects . Task , exception : Exception , ) -> None Possible use cases include: Send a log message Create a metrics collector Clean up the task's traces Call retry / requeue to rerun the same task","title":"on_failure"},{"location":"worker/handlers/on_failure/#on_failure","text":"The on_failure handler is called when a task raises an exception. The exception object is passed to the handler.","title":"on_failure"},{"location":"worker/handlers/on_failure/#definition","text":"1 2 3 4 5 def on_failure ( self , task : sergeant . objects . Task , exception : Exception , ) -> None Possible use cases include: Send a log message Create a metrics collector Clean up the task's traces Call retry / requeue to rerun the same task","title":"Definition"},{"location":"worker/handlers/on_max_retries/","text":"on_max_retries # If a task calls the retry method more than the allowed number of times, the on_max_retries handler is invoked. Definition # 1 2 3 4 def on_max_retries ( self , task : sergeant . objects . Task , ) -> None Possible use cases include: Send a log message Create a metrics collector","title":"on_max_retries"},{"location":"worker/handlers/on_max_retries/#on_max_retries","text":"If a task calls the retry method more than the allowed number of times, the on_max_retries handler is invoked.","title":"on_max_retries"},{"location":"worker/handlers/on_max_retries/#definition","text":"1 2 3 4 def on_max_retries ( self , task : sergeant . objects . Task , ) -> None Possible use cases include: Send a log message Create a metrics collector","title":"Definition"},{"location":"worker/handlers/on_requeue/","text":"on_requeue # The on_requeue handler is invoked when a task called the requeue method. Definition # 1 2 3 4 def on_requeue ( self , task : sergeant . objects . Task , ) -> None Possible use cases include: Send a log message Create a metrics collector","title":"on_requeue"},{"location":"worker/handlers/on_requeue/#on_requeue","text":"The on_requeue handler is invoked when a task called the requeue method.","title":"on_requeue"},{"location":"worker/handlers/on_requeue/#definition","text":"1 2 3 4 def on_requeue ( self , task : sergeant . objects . Task , ) -> None Possible use cases include: Send a log message Create a metrics collector","title":"Definition"},{"location":"worker/handlers/on_retry/","text":"on_retry # The on_retry handler is invoked when a task called the retry method. Definition # 1 2 3 4 def on_retry ( self , task : sergeant . objects . Task , ) -> None Possible use cases include: Send a log message Create a metrics collector","title":"on_retry"},{"location":"worker/handlers/on_retry/#on_retry","text":"The on_retry handler is invoked when a task called the retry method.","title":"on_retry"},{"location":"worker/handlers/on_retry/#definition","text":"1 2 3 4 def on_retry ( self , task : sergeant . objects . Task , ) -> None Possible use cases include: Send a log message Create a metrics collector","title":"Definition"},{"location":"worker/handlers/on_starvation/","text":"on_starvation # The on_starvation handler is invoked when a worker becomes starved. This means that there were no tasks in the broker queue and it could not pull tasks from the broker. The starvation handler is only invoked when the starvation field is configured on the worker. time_with_no_tasks indicates after how many seconds the worker should trigger this handler without tasks. Definition # 1 2 3 4 5 def on_starvation ( self , time_with_no_tasks : int , ) -> None : pass Possible use cases include: Send a log message Hint an external autoscaler to reduce the number of workers in the system The worker is no longer needed, so call stop to kill it Respawn the worker process by calling respawn , which allows memory to be released and the process to start over","title":"on_starvation"},{"location":"worker/handlers/on_starvation/#on_starvation","text":"The on_starvation handler is invoked when a worker becomes starved. This means that there were no tasks in the broker queue and it could not pull tasks from the broker. The starvation handler is only invoked when the starvation field is configured on the worker. time_with_no_tasks indicates after how many seconds the worker should trigger this handler without tasks.","title":"on_starvation"},{"location":"worker/handlers/on_starvation/#definition","text":"1 2 3 4 5 def on_starvation ( self , time_with_no_tasks : int , ) -> None : pass Possible use cases include: Send a log message Hint an external autoscaler to reduce the number of workers in the system The worker is no longer needed, so call stop to kill it Respawn the worker process by calling respawn , which allows memory to be released and the process to start over","title":"Definition"},{"location":"worker/handlers/on_success/","text":"on_success # The on_success handler is invoked when a task has completed successfully. The returned value will be passed to the handler. Definition # 1 2 3 4 5 def on_success ( self , task : sergeant . objects . Task , returned_value : typing . Any , ) -> None When the task's work method has finished successfully - without any exception being raised, without any retry attempts, this handler will be invoked. The returned value of the task will be passed to the handler. Possible use cases include: Send a log message Create a metrics collector","title":"on_success"},{"location":"worker/handlers/on_success/#on_success","text":"The on_success handler is invoked when a task has completed successfully. The returned value will be passed to the handler.","title":"on_success"},{"location":"worker/handlers/on_success/#definition","text":"1 2 3 4 5 def on_success ( self , task : sergeant . objects . Task , returned_value : typing . Any , ) -> None When the task's work method has finished successfully - without any exception being raised, without any retry attempts, this handler will be invoked. The returned value of the task will be passed to the handler. Possible use cases include: Send a log message Create a metrics collector","title":"Definition"},{"location":"worker/handlers/on_timeout/","text":"on_timeout # Upon timeout, the on_timeout handler is invoked. Unlike other events, this one is triggered by the Killer and not by something the process has done. Definition # 1 2 3 4 def on_timeout ( self , task : sergeant . objects . Task , ) -> None Possible use cases include: Send a log message Create a metrics collector Clean up the task's traces Call retry / requeue to rerun the same task","title":"on_timeout"},{"location":"worker/handlers/on_timeout/#on_timeout","text":"Upon timeout, the on_timeout handler is invoked. Unlike other events, this one is triggered by the Killer and not by something the process has done.","title":"on_timeout"},{"location":"worker/handlers/on_timeout/#definition","text":"1 2 3 4 def on_timeout ( self , task : sergeant . objects . Task , ) -> None Possible use cases include: Send a log message Create a metrics collector Clean up the task's traces Call retry / requeue to rerun the same task","title":"Definition"},{"location":"worker/lock/acquire/","text":"acquire # The acquire method attempts to acquire the lock. It allows the user to wait for the lock to become available. This method returns True when the lock is acquired, otherwise it returns False . timeout : The maximum number of seconds to wait until the lock becomes available. check_interval : How often the lock should be checked (in seconds). ttl : How long should the lock last (in seconds) after it has been acquired. Definition # 1 2 3 4 5 6 def acquire ( self , timeout : typing . Optional [ float ] = None , check_interval : float = 1.0 , ttl : int = 60 , ) -> bool Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) acquired = lock . acquire ( timeout = 10 , check_interval = 1.0 , ttl = 60 , ) if acquired : try : response = requests . get ( single_access_api_url ) finally : lock . release ()","title":"acquire"},{"location":"worker/lock/acquire/#acquire","text":"The acquire method attempts to acquire the lock. It allows the user to wait for the lock to become available. This method returns True when the lock is acquired, otherwise it returns False . timeout : The maximum number of seconds to wait until the lock becomes available. check_interval : How often the lock should be checked (in seconds). ttl : How long should the lock last (in seconds) after it has been acquired.","title":"acquire"},{"location":"worker/lock/acquire/#definition","text":"1 2 3 4 5 6 def acquire ( self , timeout : typing . Optional [ float ] = None , check_interval : float = 1.0 , ttl : int = 60 , ) -> bool","title":"Definition"},{"location":"worker/lock/acquire/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) acquired = lock . acquire ( timeout = 10 , check_interval = 1.0 , ttl = 60 , ) if acquired : try : response = requests . get ( single_access_api_url ) finally : lock . release ()","title":"Examples"},{"location":"worker/lock/get_ttl/","text":"get_ttl # get_ttl returns how many seconds remain until the lock expires. An expired lock might not be lockable. It is possible that another worker is waiting for the lock to expire. It might be helpful to call this method in order to determine whether to wait for the lock or move on to another task. This method returns the number of seconds left until the lock expires, or None if there is no lock or it has already expired. Definition # 1 2 3 def get_ttl ( self , ) -> typing . Optional [ int ] Examples # Simple 1 2 3 4 5 6 7 8 9 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) if lock . get_ttl () > 10 : self . requeue ()","title":"get_ttl"},{"location":"worker/lock/get_ttl/#get_ttl","text":"get_ttl returns how many seconds remain until the lock expires. An expired lock might not be lockable. It is possible that another worker is waiting for the lock to expire. It might be helpful to call this method in order to determine whether to wait for the lock or move on to another task. This method returns the number of seconds left until the lock expires, or None if there is no lock or it has already expired.","title":"get_ttl"},{"location":"worker/lock/get_ttl/#definition","text":"1 2 3 def get_ttl ( self , ) -> typing . Optional [ int ]","title":"Definition"},{"location":"worker/lock/get_ttl/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) if lock . get_ttl () > 10 : self . requeue ()","title":"Examples"},{"location":"worker/lock/is_locked/","text":"is_locked # is_locked returns whether the lock is locked or unlocked. Definition # 1 2 3 def is_locked ( self , ) -> bool Examples # Simple 1 2 3 4 5 6 7 8 9 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) if lock . is_locked (): self . requeue ()","title":"is_locked"},{"location":"worker/lock/is_locked/#is_locked","text":"is_locked returns whether the lock is locked or unlocked.","title":"is_locked"},{"location":"worker/lock/is_locked/#definition","text":"1 2 3 def is_locked ( self , ) -> bool","title":"Definition"},{"location":"worker/lock/is_locked/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) if lock . is_locked (): self . requeue ()","title":"Examples"},{"location":"worker/lock/release/","text":"release # This method releases the lock, allowing other workers to acquire it. The return value is True if the lock has been released, otherwise it is False . You should never release a lock unless you are the owner If a worker calls 'release' on a lock it did not acquire, the lock is released, and is made available again, even if another worker is holding it. Make sure you own the lock. Definition # 1 2 3 def release ( self , ) -> bool Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) acquired = lock . acquire ( timeout = 10 , check_interval = 1.0 , ttl = 60 , ) if acquired : try : response = requests . get ( single_access_api_url ) finally : lock . release ()","title":"release"},{"location":"worker/lock/release/#release","text":"This method releases the lock, allowing other workers to acquire it. The return value is True if the lock has been released, otherwise it is False . You should never release a lock unless you are the owner If a worker calls 'release' on a lock it did not acquire, the lock is released, and is made available again, even if another worker is holding it. Make sure you own the lock.","title":"release"},{"location":"worker/lock/release/#definition","text":"1 2 3 def release ( self , ) -> bool","title":"Definition"},{"location":"worker/lock/release/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) acquired = lock . acquire ( timeout = 10 , check_interval = 1.0 , ttl = 60 , ) if acquired : try : response = requests . get ( single_access_api_url ) finally : lock . release ()","title":"Examples"},{"location":"worker/lock/set_ttl/","text":"set_ttl # This function sets the lock time to live (in seconds). The function will return False if there is no lock with that name, otherwise it will return True . It could be useful for extending the lock's lifetime. Locks are automatically released when their TTL is exceeded without explicitly calling release . Definition # 1 2 3 4 def set_ttl ( self , ttl : int , ) -> bool Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) if self . acquire ( timeout = 10 , ttl = 20 , ): while True : result = some_api () if result . not_yet_available : lock . set_ttl ( 20 ) else : finalize ( result )","title":"set_ttl"},{"location":"worker/lock/set_ttl/#set_ttl","text":"This function sets the lock time to live (in seconds). The function will return False if there is no lock with that name, otherwise it will return True . It could be useful for extending the lock's lifetime. Locks are automatically released when their TTL is exceeded without explicitly calling release .","title":"set_ttl"},{"location":"worker/lock/set_ttl/#definition","text":"1 2 3 4 def set_ttl ( self , ttl : int , ) -> bool","title":"Definition"},{"location":"worker/lock/set_ttl/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) if self . acquire ( timeout = 10 , ttl = 20 , ): while True : result = some_api () if result . not_yet_available : lock . set_ttl ( 20 ) else : finalize ( result )","title":"Examples"},{"location":"worker/logging/logstash/","text":"logstash # Sergeant implements logging handlers to support logging platforms with high performance. We implemented for logstash two different handlers: LogstashHandler - Logstash TCP handler. A TCP socket is used to send log messages to logstash, and orjson is used to serialize the data as JSON. BufferedLogstashHandler - The same as LogstashHandler except that it manages an internal buffer and flushes it according to certain rules. Following is an example of logstash server configuration: 1 2 3 4 5 6 input { tcp { port => 9999 codec => json_lines } } Definition # LogstashHandler # 1 2 3 4 5 6 7 8 9 class LogstashHandler ( BaseLogstashHandler , ): def __init__ ( self , host : str , port : int , timeout : typing . Optional [ float ] = 2.0 , ) -> None - host - The hostname or IP address of the logstash server. - port - The port of the logstash server's input. - timeout - Global socket timeout. A timeout will be used if the server does not respond within a certain amount of time. BufferedLogstashHandler # 1 2 3 4 5 6 7 8 9 10 11 class BufferedLogstashHandler ( BaseLogstashHandler , ): def __init__ ( self , host : str , port : int , timeout : typing . Optional [ float ] = 2.0 , chunk_size : int = 100 , max_store_time : float = 60.0 , ) -> None - host - The hostname or IP address of the logstash server. - port - The port of the logstash server's input. - timeout - Global socket timeout. A timeout will be used if the server does not respond within a certain amount of time. - chunk_size - The number of log entries to store before sending over a single connection to logstash. - max_store_time - The amount of time that should elapse before sending the available messages to logstash, even if the queue has not reached the chunk_size . Examples # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import logging import sergeant class Worker ( sergeant . worker . Worker , ): def generate_config ( self , ): return sergeant . config . WorkerConfig ( name = 'test_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), logging = sergeant . config . Logging ( handlers = [ sergeant . logging . logstash . LogstashHandler ( host = 'localhost' , port = 9999 , ), ], ), )","title":"logstash"},{"location":"worker/logging/logstash/#logstash","text":"Sergeant implements logging handlers to support logging platforms with high performance. We implemented for logstash two different handlers: LogstashHandler - Logstash TCP handler. A TCP socket is used to send log messages to logstash, and orjson is used to serialize the data as JSON. BufferedLogstashHandler - The same as LogstashHandler except that it manages an internal buffer and flushes it according to certain rules. Following is an example of logstash server configuration: 1 2 3 4 5 6 input { tcp { port => 9999 codec => json_lines } }","title":"logstash"},{"location":"worker/logging/logstash/#definition","text":"","title":"Definition"},{"location":"worker/logging/logstash/#logstashhandler","text":"1 2 3 4 5 6 7 8 9 class LogstashHandler ( BaseLogstashHandler , ): def __init__ ( self , host : str , port : int , timeout : typing . Optional [ float ] = 2.0 , ) -> None - host - The hostname or IP address of the logstash server. - port - The port of the logstash server's input. - timeout - Global socket timeout. A timeout will be used if the server does not respond within a certain amount of time.","title":"LogstashHandler"},{"location":"worker/logging/logstash/#bufferedlogstashhandler","text":"1 2 3 4 5 6 7 8 9 10 11 class BufferedLogstashHandler ( BaseLogstashHandler , ): def __init__ ( self , host : str , port : int , timeout : typing . Optional [ float ] = 2.0 , chunk_size : int = 100 , max_store_time : float = 60.0 , ) -> None - host - The hostname or IP address of the logstash server. - port - The port of the logstash server's input. - timeout - Global socket timeout. A timeout will be used if the server does not respond within a certain amount of time. - chunk_size - The number of log entries to store before sending over a single connection to logstash. - max_store_time - The amount of time that should elapse before sending the available messages to logstash, even if the queue has not reached the chunk_size .","title":"BufferedLogstashHandler"},{"location":"worker/logging/logstash/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import logging import sergeant class Worker ( sergeant . worker . Worker , ): def generate_config ( self , ): return sergeant . config . WorkerConfig ( name = 'test_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), logging = sergeant . config . Logging ( handlers = [ sergeant . logging . logstash . LogstashHandler ( host = 'localhost' , port = 9999 , ), ], ), )","title":"Examples"},{"location":"worker/methods/get_next_tasks/","text":"get_next_tasks # get_next_tasks method pulls number_of_tasks tasks from the queue. Uses current worker name unless task_name was specified. It is not recommended that anyone use this function directly unless they are familiar with it. Definition # 1 2 3 4 5 def get_next_tasks ( self , number_of_tasks : int , task_name : typing . Optional [ str ] = None , ) -> typing . List [ typing . Dict [ str , typing . Any ]] Examples # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def work ( self , task , ): statistics = { 'google.com' : 0 , 'facebook.com' : 0 , } while True : tasks = self . get_next_tasks ( number_of_tasks = 1000 , task_name = 'unfiltered_task' , ) if not tasks : break for task in tasks : domain = task . kwargs [ 'domain' ] if domain not in statistics : continue statistics [ domain ] += 1 self . push_task ( kwargs = { 'domain' : domain , 'params' : task . kwargs [ 'params' ] }, task_name = 'filtered_task' , )","title":"get_next_tasks"},{"location":"worker/methods/get_next_tasks/#get_next_tasks","text":"get_next_tasks method pulls number_of_tasks tasks from the queue. Uses current worker name unless task_name was specified. It is not recommended that anyone use this function directly unless they are familiar with it.","title":"get_next_tasks"},{"location":"worker/methods/get_next_tasks/#definition","text":"1 2 3 4 5 def get_next_tasks ( self , number_of_tasks : int , task_name : typing . Optional [ str ] = None , ) -> typing . List [ typing . Dict [ str , typing . Any ]]","title":"Definition"},{"location":"worker/methods/get_next_tasks/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def work ( self , task , ): statistics = { 'google.com' : 0 , 'facebook.com' : 0 , } while True : tasks = self . get_next_tasks ( number_of_tasks = 1000 , task_name = 'unfiltered_task' , ) if not tasks : break for task in tasks : domain = task . kwargs [ 'domain' ] if domain not in statistics : continue statistics [ domain ] += 1 self . push_task ( kwargs = { 'domain' : domain , 'params' : task . kwargs [ 'params' ] }, task_name = 'filtered_task' , )","title":"Examples"},{"location":"worker/methods/lock/","text":"lock # A distributed lock is created by the lock method and stored on the broker. With the help of a distributed lock, multiple workers can synchronize their work. Distributed locks can be used for many different purposes, such as locking access to a throttled resource so that it does not exceed its rate limit. Definition # 1 2 3 4 def lock ( self , name : str , ) -> typing . Union [ connector . mongo . Lock , connector . redis . Lock ]: Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) acquired = lock . acquire ( timeout = 10 , ttl = 60 , ) if acquired : try : response = requests . get ( single_access_api_url ) finally : lock . release ()","title":"lock"},{"location":"worker/methods/lock/#lock","text":"A distributed lock is created by the lock method and stored on the broker. With the help of a distributed lock, multiple workers can synchronize their work. Distributed locks can be used for many different purposes, such as locking access to a throttled resource so that it does not exceed its rate limit.","title":"lock"},{"location":"worker/methods/lock/#definition","text":"1 2 3 4 def lock ( self , name : str , ) -> typing . Union [ connector . mongo . Lock , connector . redis . Lock ]:","title":"Definition"},{"location":"worker/methods/lock/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) acquired = lock . acquire ( timeout = 10 , ttl = 60 , ) if acquired : try : response = requests . get ( single_access_api_url ) finally : lock . release ()","title":"Examples"},{"location":"worker/methods/number_of_enqueued_tasks/","text":"number_of_enqueued_tasks # This method returns a count of all tasks queued in the queue. The current worker name will be used unless task_name was specified. include_delayed determines whether to count delayed tasks as well. If you implement an autoscaler, and you want to scale the number of workers based on the number of consumable tasks rather than the number of consumable tasks combined with delayed tasks, then you should use the include_delayed parameter. Definition # 1 2 3 4 5 def number_of_enqueued_tasks ( self , task_name : typing . Optional [ str ] = None , include_delayed : bool = False , ) -> int Examples # 1 2 3 4 5 6 7 def work ( self , task , ): number_of_enqueue_tasks = self . number_of_enqueued_tasks () if number_of_enqueue_tasks > 100000 : self . autoscaler . increase_scaling ()","title":"number_of_enqueued_tasks"},{"location":"worker/methods/number_of_enqueued_tasks/#number_of_enqueued_tasks","text":"This method returns a count of all tasks queued in the queue. The current worker name will be used unless task_name was specified. include_delayed determines whether to count delayed tasks as well. If you implement an autoscaler, and you want to scale the number of workers based on the number of consumable tasks rather than the number of consumable tasks combined with delayed tasks, then you should use the include_delayed parameter.","title":"number_of_enqueued_tasks"},{"location":"worker/methods/number_of_enqueued_tasks/#definition","text":"1 2 3 4 5 def number_of_enqueued_tasks ( self , task_name : typing . Optional [ str ] = None , include_delayed : bool = False , ) -> int","title":"Definition"},{"location":"worker/methods/number_of_enqueued_tasks/#examples","text":"1 2 3 4 5 6 7 def work ( self , task , ): number_of_enqueue_tasks = self . number_of_enqueued_tasks () if number_of_enqueue_tasks > 100000 : self . autoscaler . increase_scaling ()","title":"Examples"},{"location":"worker/methods/purge_tasks/","text":"purge_tasks # purge_tasks removes all tasks from the queue. It allows workers to implement some critical functionality in cases where it identifies a situation that should prevent future tasks from being executed. The current worker name will be used unless task_name was specified. Definition # 1 2 3 4 def purge_tasks ( self , task_name : typing . Optional [ str ] = None , ) -> bool Examples # 1 2 3 4 5 6 7 8 9 10 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . purge_tasks ()","title":"purge_tasks"},{"location":"worker/methods/purge_tasks/#purge_tasks","text":"purge_tasks removes all tasks from the queue. It allows workers to implement some critical functionality in cases where it identifies a situation that should prevent future tasks from being executed. The current worker name will be used unless task_name was specified.","title":"purge_tasks"},{"location":"worker/methods/purge_tasks/#definition","text":"1 2 3 4 def purge_tasks ( self , task_name : typing . Optional [ str ] = None , ) -> bool","title":"Definition"},{"location":"worker/methods/purge_tasks/#examples","text":"1 2 3 4 5 6 7 8 9 10 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . purge_tasks ()","title":"Examples"},{"location":"worker/methods/push_task/","text":"push_task # push_task pushes a task onto the queue. The current worker name will be used unless task_name was specified. kwargs - A dictionary of serializable arguments to pass to the worker. task_name - The name of the task/queue to push to. priority : NORMAL - The task will be placed at the top of the queue. It will be pulled last. [FIFO] HIGH - The task will be placed at the bottom of the queue. It will be pulled first. [LIFO] consumable_from - The Unix time represents the point in time when the task becomes consumable and can be popped from the queue. 0 means now. Definition # 1 2 3 4 5 6 7 def push_task ( self , kwargs : typing . Dict [ str , typing . Any ], task_name : typing . Optional [ str ] = None , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> bool Examples # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . retry ( task = task , consumable_from = int ( time . time () + 60 * 30 ), ) else : self . push_task ( kwargs = { 'html' : response . content , }, task_name = 'parse_html' , priority = 'NORMAL' , )","title":"push_task"},{"location":"worker/methods/push_task/#push_task","text":"push_task pushes a task onto the queue. The current worker name will be used unless task_name was specified. kwargs - A dictionary of serializable arguments to pass to the worker. task_name - The name of the task/queue to push to. priority : NORMAL - The task will be placed at the top of the queue. It will be pulled last. [FIFO] HIGH - The task will be placed at the bottom of the queue. It will be pulled first. [LIFO] consumable_from - The Unix time represents the point in time when the task becomes consumable and can be popped from the queue. 0 means now.","title":"push_task"},{"location":"worker/methods/push_task/#definition","text":"1 2 3 4 5 6 7 def push_task ( self , kwargs : typing . Dict [ str , typing . Any ], task_name : typing . Optional [ str ] = None , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> bool","title":"Definition"},{"location":"worker/methods/push_task/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . retry ( task = task , consumable_from = int ( time . time () + 60 * 30 ), ) else : self . push_task ( kwargs = { 'html' : response . content , }, task_name = 'parse_html' , priority = 'NORMAL' , )","title":"Examples"},{"location":"worker/methods/push_tasks/","text":"push_tasks # This method bulk inserts multiple tasks into the queue at once. The current worker name will be used unless task_name was specified. Unlike push_task , this method gets a list of kwargs and pushes much faster. kwargs_list - A list of dictionaries of serializable arguments to pass to the worker. task_name - The name of the task/queue to push to. priority : NORMAL - The task will be placed at the top of the queue. It will be pulled last. [FIFO] HIGH - The task will be placed at the bottom of the queue. It will be pulled first. [LIFO] consumable_from - The Unix time represents the point in time when the task becomes consumable and can be popped from the queue. 0 means now. Definition # 1 2 3 4 5 6 7 def push_tasks ( self , kwargs_list : typing . Iterable [ typing . Dict [ str , typing . Any ]], task_name : typing . Optional [ str ] = None , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> bool Examples # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . retry ( task = task , consumable_from = int ( time . time () + 60 * 30 ), ) urls = self . extract_urls ( response . content ) self . push_tasks ( kwargs_list = [ { 'url' : url , } for url in urls ], task_name = 'crawl_url' , priority = 'NORMAL' , )","title":"push_tasks"},{"location":"worker/methods/push_tasks/#push_tasks","text":"This method bulk inserts multiple tasks into the queue at once. The current worker name will be used unless task_name was specified. Unlike push_task , this method gets a list of kwargs and pushes much faster. kwargs_list - A list of dictionaries of serializable arguments to pass to the worker. task_name - The name of the task/queue to push to. priority : NORMAL - The task will be placed at the top of the queue. It will be pulled last. [FIFO] HIGH - The task will be placed at the bottom of the queue. It will be pulled first. [LIFO] consumable_from - The Unix time represents the point in time when the task becomes consumable and can be popped from the queue. 0 means now.","title":"push_tasks"},{"location":"worker/methods/push_tasks/#definition","text":"1 2 3 4 5 6 7 def push_tasks ( self , kwargs_list : typing . Iterable [ typing . Dict [ str , typing . Any ]], task_name : typing . Optional [ str ] = None , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> bool","title":"Definition"},{"location":"worker/methods/push_tasks/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . retry ( task = task , consumable_from = int ( time . time () + 60 * 30 ), ) urls = self . extract_urls ( response . content ) self . push_tasks ( kwargs_list = [ { 'url' : url , } for url in urls ], task_name = 'crawl_url' , priority = 'NORMAL' , )","title":"Examples"},{"location":"worker/methods/requeue/","text":"requeue # Using this method, the task is pushed back to the queue without increasing the its run_count . It works by raising a WorkerRequeue exception, which causes the worker to interrupt. It means you should call requeue only if there is nothing else to do. If you call requeue and catch the exception, the worker will not be interrupted, but the task will still be returned to the queue. Tasks that are requeued are considered failed tasks. consumable_from is the Unix time represents the point in time when the task becomes consumable and can be popped from the queue. 0 means now. Requeue from handlers requeue should never be called from the following handlers: on_success on_retry on_max_retries on_requeue Definition # 1 2 3 4 5 6 def requeue ( self , task : sergeant . objects . Task , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> None Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . requeue ( task = task , ) OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) response . raise_for_status () def on_failure ( self , task , exception , ): self . requeue ( task = task , )","title":"requeue"},{"location":"worker/methods/requeue/#requeue","text":"Using this method, the task is pushed back to the queue without increasing the its run_count . It works by raising a WorkerRequeue exception, which causes the worker to interrupt. It means you should call requeue only if there is nothing else to do. If you call requeue and catch the exception, the worker will not be interrupted, but the task will still be returned to the queue. Tasks that are requeued are considered failed tasks. consumable_from is the Unix time represents the point in time when the task becomes consumable and can be popped from the queue. 0 means now. Requeue from handlers requeue should never be called from the following handlers: on_success on_retry on_max_retries on_requeue","title":"requeue"},{"location":"worker/methods/requeue/#definition","text":"1 2 3 4 5 6 def requeue ( self , task : sergeant . objects . Task , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> None","title":"Definition"},{"location":"worker/methods/requeue/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . requeue ( task = task , ) OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) response . raise_for_status () def on_failure ( self , task , exception , ): self . requeue ( task = task , )","title":"Examples"},{"location":"worker/methods/respawn/","text":"respawn # Respawning the worker's process is accomplished using the respawn method. The supervisor receives the exception, and initiates a new worker process instead. The method can be used to kill the current worker process and spawn a new one. It can be used to restart a process after it has entered an irrecoverable state (e.g. a memory leak). Definition # 1 2 3 def respawn ( self , ) -> None Examples # Simple 1 2 3 4 5 6 7 8 9 10 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if self . memory_usage () > self . max_memory_usage : self . respawn ()","title":"respawn"},{"location":"worker/methods/respawn/#respawn","text":"Respawning the worker's process is accomplished using the respawn method. The supervisor receives the exception, and initiates a new worker process instead. The method can be used to kill the current worker process and spawn a new one. It can be used to restart a process after it has entered an irrecoverable state (e.g. a memory leak).","title":"respawn"},{"location":"worker/methods/respawn/#definition","text":"1 2 3 def respawn ( self , ) -> None","title":"Definition"},{"location":"worker/methods/respawn/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if self . memory_usage () > self . max_memory_usage : self . respawn ()","title":"Examples"},{"location":"worker/methods/retry/","text":"retry # Retrying a task pushes it back into the queue while increasing its run count by one. It works by raising a WorkerRetry exception, which causes the worker to interrupt. Therefore, you should only call retry if you don't have any more work to do. If you call retry and catch the exception, the worker will not be interrupted but the task will be pushed back to the queue anyway. Retrying the task makes it a failed task. consumable_from is the Unix time represents the point in time when the task becomes consumable and can be popped from the queue. 0 means now. A temporary failure can be retried at a later time using the consumable_from parameter. Retry from handlers retry should never be called from the following handlers: on_success on_retry on_max_retries on_requeue Definition # 1 2 3 4 5 6 def retry ( self , task : sergeant . objects . Task , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> None Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . retry ( task = task , ) OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) response . raise_for_status () def on_failure ( self , task , exception , ): self . retry ( task = task , )","title":"retry"},{"location":"worker/methods/retry/#retry","text":"Retrying a task pushes it back into the queue while increasing its run count by one. It works by raising a WorkerRetry exception, which causes the worker to interrupt. Therefore, you should only call retry if you don't have any more work to do. If you call retry and catch the exception, the worker will not be interrupted but the task will be pushed back to the queue anyway. Retrying the task makes it a failed task. consumable_from is the Unix time represents the point in time when the task becomes consumable and can be popped from the queue. 0 means now. A temporary failure can be retried at a later time using the consumable_from parameter. Retry from handlers retry should never be called from the following handlers: on_success on_retry on_max_retries on_requeue","title":"retry"},{"location":"worker/methods/retry/#definition","text":"1 2 3 4 5 6 def retry ( self , task : sergeant . objects . Task , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> None","title":"Definition"},{"location":"worker/methods/retry/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . retry ( task = task , ) OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) response . raise_for_status () def on_failure ( self , task , exception , ): self . retry ( task = task , )","title":"Examples"},{"location":"worker/methods/stop/","text":"stop # It is possible to stop the worker from running by using the stop method. The supervisor receives the request, and will never spawn a new worker instead. A worker uses this method when there is no solution to a problem. Using this method in conjunction with on_starvation is a good example of how it should be used. If a worker is starving and there are not enough tasks available to consume, it can be intentionally stopped to reduce the load on the queue. By calling stop , all the workers under a specific supervisor will stop as well. Definition # 1 2 3 def stop ( self , ) -> None Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) try : self . database . insert_crawl_result ( url = url_to_crawl , content = response . content , ) except pymongo . errors . ServerSelectionTimeoutError : self . stop () OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) self . database . insert_crawl_result ( url = url_to_crawl , content = response . content , ) def on_failure ( self , task , exception , ): if isinstance ( exception , pymongo . errors . ServerSelectionTimeoutError ): self . stop ()","title":"stop"},{"location":"worker/methods/stop/#stop","text":"It is possible to stop the worker from running by using the stop method. The supervisor receives the request, and will never spawn a new worker instead. A worker uses this method when there is no solution to a problem. Using this method in conjunction with on_starvation is a good example of how it should be used. If a worker is starving and there are not enough tasks available to consume, it can be intentionally stopped to reduce the load on the queue. By calling stop , all the workers under a specific supervisor will stop as well.","title":"stop"},{"location":"worker/methods/stop/#definition","text":"1 2 3 def stop ( self , ) -> None","title":"Definition"},{"location":"worker/methods/stop/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) try : self . database . insert_crawl_result ( url = url_to_crawl , content = response . content , ) except pymongo . errors . ServerSelectionTimeoutError : self . stop () OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) self . database . insert_crawl_result ( url = url_to_crawl , content = response . content , ) def on_failure ( self , task , exception , ): if isinstance ( exception , pymongo . errors . ServerSelectionTimeoutError ): self . stop ()","title":"Examples"},{"location":"worker/worker/finalize/","text":"finalize # The finalize method is invoked when the maximum number of tasks per run has been reached. It is invoked only once. This is a good time for collecting metrics, closing handles, and performing cleanups. Definition # 1 2 3 def finalize ( self , ) -> None Examples # 1 2 3 4 5 def finalize ( self , ): self . apm_client . close () self . mongo . close ()","title":"finalize"},{"location":"worker/worker/finalize/#finalize","text":"The finalize method is invoked when the maximum number of tasks per run has been reached. It is invoked only once. This is a good time for collecting metrics, closing handles, and performing cleanups.","title":"finalize"},{"location":"worker/worker/finalize/#definition","text":"1 2 3 def finalize ( self , ) -> None","title":"Definition"},{"location":"worker/worker/finalize/#examples","text":"1 2 3 4 5 def finalize ( self , ): self . apm_client . close () self . mongo . close ()","title":"Examples"},{"location":"worker/worker/generate_config/","text":"generate_config # With this method, the user returns a configuration that defines how the worker should be configured. Methods allow more flexibility for dynamic configurations than class attributes, which are interpreted during module load time. Definition # 1 2 3 def generate_config ( self , ) -> config . WorkerConfig : Examples # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def generate_config ( self , ) -> config . WorkerConfig : redis_configuration = requests . get ( url = 'https://redis-load-balancer/get-instance' , ) . json () return config . WorkerConfig ( name = 'worker_name' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : redis_configuration [ 'host' ], 'port' : redis_configuration [ 'port' ], 'password' : redis_configuration [ 'password' ], 'database' : redis_configuration [ 'database' ], }, ], }, ), )","title":"generate_config"},{"location":"worker/worker/generate_config/#generate_config","text":"With this method, the user returns a configuration that defines how the worker should be configured. Methods allow more flexibility for dynamic configurations than class attributes, which are interpreted during module load time.","title":"generate_config"},{"location":"worker/worker/generate_config/#definition","text":"1 2 3 def generate_config ( self , ) -> config . WorkerConfig :","title":"Definition"},{"location":"worker/worker/generate_config/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def generate_config ( self , ) -> config . WorkerConfig : redis_configuration = requests . get ( url = 'https://redis-load-balancer/get-instance' , ) . json () return config . WorkerConfig ( name = 'worker_name' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : redis_configuration [ 'host' ], 'port' : redis_configuration [ 'port' ], 'password' : redis_configuration [ 'password' ], 'database' : redis_configuration [ 'database' ], }, ], }, ), )","title":"Examples"},{"location":"worker/worker/initialize/","text":"initialize # The worker invokes the initialize method once when it is spawned by the supervisor. By using this method, you will be able to initialize objects that will last for the lifetime of the worker. The initialization of a Database connection, Logger or APM object is a good use case. Definition # 1 2 3 def initialize ( self , ) -> None Examples # 1 2 3 4 5 def initialize ( self , ): self . my_logger = logging . getLogger () self . apm_client = elasticapm . Client ()","title":"initialize"},{"location":"worker/worker/initialize/#initialize","text":"The worker invokes the initialize method once when it is spawned by the supervisor. By using this method, you will be able to initialize objects that will last for the lifetime of the worker. The initialization of a Database connection, Logger or APM object is a good use case.","title":"initialize"},{"location":"worker/worker/initialize/#definition","text":"1 2 3 def initialize ( self , ) -> None","title":"Definition"},{"location":"worker/worker/initialize/#examples","text":"1 2 3 4 5 def initialize ( self , ): self . my_logger = logging . getLogger () self . apm_client = elasticapm . Client ()","title":"Examples"},{"location":"worker/worker/post_work/","text":"post_work # After a task has been completed, the worker calls the post_work method. This method allows the user to perform an operation after each task is completed. You can use it to close an APM transaction or to send a metric. Definition # 1 2 3 4 5 6 def post_work ( self , task : sergeant . objects . Task , success : bool , exception : typing . Optional [ BaseException ], ) -> None Examples # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def post_work ( self , task , success , exception , ): self . my_logger . debug ( f 'stopped working on { task . kwargs [ \"url\" ] } : { time . time () } , exception: { exception } ' ) if exception is not None : self . apm_client . capture_exception () self . apm_client . end_transaction ( result = 'success' if success else 'failure' , )","title":"post_work"},{"location":"worker/worker/post_work/#post_work","text":"After a task has been completed, the worker calls the post_work method. This method allows the user to perform an operation after each task is completed. You can use it to close an APM transaction or to send a metric.","title":"post_work"},{"location":"worker/worker/post_work/#definition","text":"1 2 3 4 5 6 def post_work ( self , task : sergeant . objects . Task , success : bool , exception : typing . Optional [ BaseException ], ) -> None","title":"Definition"},{"location":"worker/worker/post_work/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 def post_work ( self , task , success , exception , ): self . my_logger . debug ( f 'stopped working on { task . kwargs [ \"url\" ] } : { time . time () } , exception: { exception } ' ) if exception is not None : self . apm_client . capture_exception () self . apm_client . end_transaction ( result = 'success' if success else 'failure' , )","title":"Examples"},{"location":"worker/worker/pre_work/","text":"pre_work # Before the work method is invoked for each execution of a task, the worker invokes the pre_work method. Through this method, the user can perform an operation prior to each execution. You can use it to open an APM transaction, measure the time before a task, and more. There is no effect of the timeout parameter or the killer on this method. In other words, if the user runs something indefinitely here, the worker will be stuck. Definition # 1 2 3 4 def pre_work ( self , task : sergeant . objects . Task , ) -> None Examples # 1 2 3 4 5 6 def pre_work ( self , task , ): self . my_logger . debug ( f 'started working on { task . kwargs [ \"url\" ] } : { time . time () } ' ) self . apm_client . begin_transaction ()","title":"pre_work"},{"location":"worker/worker/pre_work/#pre_work","text":"Before the work method is invoked for each execution of a task, the worker invokes the pre_work method. Through this method, the user can perform an operation prior to each execution. You can use it to open an APM transaction, measure the time before a task, and more. There is no effect of the timeout parameter or the killer on this method. In other words, if the user runs something indefinitely here, the worker will be stuck.","title":"pre_work"},{"location":"worker/worker/pre_work/#definition","text":"1 2 3 4 def pre_work ( self , task : sergeant . objects . Task , ) -> None","title":"Definition"},{"location":"worker/worker/pre_work/#examples","text":"1 2 3 4 5 6 def pre_work ( self , task , ): self . my_logger . debug ( f 'started working on { task . kwargs [ \"url\" ] } : { time . time () } ' ) self . apm_client . begin_transaction ()","title":"Examples"},{"location":"worker/worker/work/","text":"work # In the work method, we should include our work logic. Here we will execute the task. It is recommended to include all the necessary information in the task input parameter so that the worker can perform its logic. It is necessary for worker methods to pass and receive the task object in order to function. 1 2 3 4 5 6 7 8 9 @dataclasses . dataclass class Task : kwargs : typing . Dict [ str , typing . Any ] = dataclasses . field ( default_factory = dict , ) date : int = dataclasses . field ( default_factory = lambda : int ( time . time ()), ) run_count : int = 0 date - This is the date the task object was created and pushed to the queue. kwargs - This is a dictionary of arguments that were passed to the worker. run_count - This is the number of times the task was executed. Definition # 1 2 3 4 def work ( self , task : sergeant . objects . Task , ) -> typing . Any Examples # 1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . retry () self . mongo . crawling_db . webpages . insert_one ( ... )","title":"work"},{"location":"worker/worker/work/#work","text":"In the work method, we should include our work logic. Here we will execute the task. It is recommended to include all the necessary information in the task input parameter so that the worker can perform its logic. It is necessary for worker methods to pass and receive the task object in order to function. 1 2 3 4 5 6 7 8 9 @dataclasses . dataclass class Task : kwargs : typing . Dict [ str , typing . Any ] = dataclasses . field ( default_factory = dict , ) date : int = dataclasses . field ( default_factory = lambda : int ( time . time ()), ) run_count : int = 0 date - This is the date the task object was created and pushed to the queue. kwargs - This is a dictionary of arguments that were passed to the worker. run_count - This is the number of times the task was executed.","title":"work"},{"location":"worker/worker/work/#definition","text":"1 2 3 4 def work ( self , task : sergeant . objects . Task , ) -> typing . Any","title":"Definition"},{"location":"worker/worker/work/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . retry () self . mongo . crawling_db . webpages . insert_one ( ... )","title":"Examples"}]}