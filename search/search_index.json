{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Fast, Safe & Simple Asynchronous Task Queues Written In Pure Python Home # Sergeant was written in Intsights after failing to use celery with large scale. Our infrastructure had peaks of more than 100k tasks a second and thousands of workers. We found Celery to be slow and unstable at that scale. Sergeant as opposed to celery is much simpler and more accessible. Instead of supporting a wide range of backends and scenarios, this library provides a simple and intuitive interface to implement a queue-based distributed workers architecture. The two supported backends are redis and mongo . As an in-memory database, Redis is recommended for high throughput environments, where performance is the top priority. Mongo is recommended for stable and consistent systems where you need your tasks to be saved on the disk. In order to keep the library simple, we decided not to support AMQP backends. The library uses some latest python features such as dataclasses and f-strings , making this library compatible only with Python 3.7 and up. The library supports multiple serializers and compressions.Before pushing a task into the queue, the library serializes the job and optionally compresses it. The supported serializers are pickle and msgpack . One should not switch the default serializer pickle unless it has some limitations, such as security concerns or portability reasons. Each serializer supports different data types. While pickle is not portable, it supports broader data types than any other serializer. Executing tasks with parameters that are not supported by msgpack requires serializing/deserializing manually or using pickle. As for the compressions, this library support multiple built-in compressions algorithms that are supported natively by Python. zlib , gzip , bzip2 and lzma . The default compressor is None which does not compress at all. Using a compressor is encouraged when you have a massive amount of tasks in your queues, and you want to spare RAM/Storage. The library tasks execution supported mechanisms are multiprocessing and threading . This means that each task might be executed according to the implementor's choice, in a different thread, or different process. The concurrency level is configurable. The library supports two watchdogs mechanisms to evade situations where the tasks are running for a longer time than expected. The first is a process killer. This watchdog spawns a separated process that waits for starts and stops signals to ensure the tasks did not exceed the specified maximum timeout. When using a threading executor, a thread killer is being spawned. Due to the nature of threads in Python, this killer has only one ability, and it is to raise an Exception in the context of the watched thread. In situations where the thread locks the GIL, like executing an infinite regex, this killer will fail at its job. Choosing a threaded executor should be taken with caution. The primary process which takes care of the worker is the supervisor . The supervisor spawns the child workers, each one in a different process, and waits for their execution to complete. Once completed, the supervisor spawns another child in place of the worker. The supervisor also takes care of an out of bound workers. A worker which violates some optional violations is being killed and replaced with another one. You can specify a memory usage limit.","title":"Home"},{"location":"#home","text":"Sergeant was written in Intsights after failing to use celery with large scale. Our infrastructure had peaks of more than 100k tasks a second and thousands of workers. We found Celery to be slow and unstable at that scale. Sergeant as opposed to celery is much simpler and more accessible. Instead of supporting a wide range of backends and scenarios, this library provides a simple and intuitive interface to implement a queue-based distributed workers architecture. The two supported backends are redis and mongo . As an in-memory database, Redis is recommended for high throughput environments, where performance is the top priority. Mongo is recommended for stable and consistent systems where you need your tasks to be saved on the disk. In order to keep the library simple, we decided not to support AMQP backends. The library uses some latest python features such as dataclasses and f-strings , making this library compatible only with Python 3.7 and up. The library supports multiple serializers and compressions.Before pushing a task into the queue, the library serializes the job and optionally compresses it. The supported serializers are pickle and msgpack . One should not switch the default serializer pickle unless it has some limitations, such as security concerns or portability reasons. Each serializer supports different data types. While pickle is not portable, it supports broader data types than any other serializer. Executing tasks with parameters that are not supported by msgpack requires serializing/deserializing manually or using pickle. As for the compressions, this library support multiple built-in compressions algorithms that are supported natively by Python. zlib , gzip , bzip2 and lzma . The default compressor is None which does not compress at all. Using a compressor is encouraged when you have a massive amount of tasks in your queues, and you want to spare RAM/Storage. The library tasks execution supported mechanisms are multiprocessing and threading . This means that each task might be executed according to the implementor's choice, in a different thread, or different process. The concurrency level is configurable. The library supports two watchdogs mechanisms to evade situations where the tasks are running for a longer time than expected. The first is a process killer. This watchdog spawns a separated process that waits for starts and stops signals to ensure the tasks did not exceed the specified maximum timeout. When using a threading executor, a thread killer is being spawned. Due to the nature of threads in Python, this killer has only one ability, and it is to raise an Exception in the context of the watched thread. In situations where the thread locks the GIL, like executing an infinite regex, this killer will fail at its job. Choosing a threaded executor should be taken with caution. The primary process which takes care of the worker is the supervisor . The supervisor spawns the child workers, each one in a different process, and waits for their execution to complete. Once completed, the supervisor spawns another child in place of the worker. The supervisor also takes care of an out of bound workers. A worker which violates some optional violations is being killed and replaced with another one. You can specify a memory usage limit.","title":"Home"},{"location":"supervisor/","text":"Supervisor # The Supervisor is in charge of spawning new workers, and respawning them when they exit. It is also responsible for handling their errors. The supervisor is capable of restricting worker's memory usage to eliminate memory exhaustion. concurrent-workers - How many subprocesses the supervisor should spawn and supervise. worker-module - The worker module in a dotted notation path. worker-class - The worker class name inside the module file - usually Worker . max-worker-memory-usage [optional] - How much RSS memory in bytes a subprocess-worker can use before the supervisor terminates it and respawn a new one instead. logger [optional - programatically only] - One can supply a custom logger to pipe all the supervisor logs to this logger. Command Line # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 python3 -m sergeant.supervisor --helpusage: supervisor.py [ -h ] --concurrent-workers CONCURRENT_WORKERS --worker-class WORKER_CLASS --worker-module WORKER_MODULE [ --max-worker-memory-usage MAX_WORKER_MEMORY_USAGE ] Sergeant Supervisor optional arguments: -h, --help show this help message and exit --concurrent-workers CONCURRENT_WORKERS Number of subprocesses to open --worker-class WORKER_CLASS Class name of the worker to spawn --worker-module WORKER_MODULE Module of the worker class --max-worker-memory-usage MAX_WORKER_MEMORY_USAGE Maximum RSS memory usage in bytes of an individual worker. When a worker reaches this value, the supevisor would kill it and respawn another one in place. Examples # Assuming a Supervisor with concurrency level of 4 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = crawl_worker \\ --worker-class = Worker \\ --concurrent-workers = 4 graph TD Supervisor --> crawl_worker-1 Supervisor --> crawl_worker-2 Supervisor --> crawl_worker-3 Supervisor --> crawl_worker-4 When a worker reaches its end of life - at the moment it finished max_tasks_per_run tasks, it will exit, and a new worker will be created by the supervisor. Programatically # It is possilbe to programatically invoke a Supervisor if you would like to document your supervisor parameters or if you would like to attach another logger. Examples # Pay attention to the worker_module_name parameter. The way python find the module name depends on the command line CWD. Look at benchmark/1_simple_worker/sergeant/supervisor.py to see how to work with module names if the supervisor.py file lays aside the worker module. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import sergeant def main (): supervisor = sergeant . supervisor . Supervisor ( worker_module_name = 'consumer' , worker_class_name = 'Worker' , concurrent_workers = 1 , max_worker_memory_usage = None , ) supervisor . start () if __name__ == '__main__' : main ()","title":"Supervisor"},{"location":"supervisor/#supervisor","text":"The Supervisor is in charge of spawning new workers, and respawning them when they exit. It is also responsible for handling their errors. The supervisor is capable of restricting worker's memory usage to eliminate memory exhaustion. concurrent-workers - How many subprocesses the supervisor should spawn and supervise. worker-module - The worker module in a dotted notation path. worker-class - The worker class name inside the module file - usually Worker . max-worker-memory-usage [optional] - How much RSS memory in bytes a subprocess-worker can use before the supervisor terminates it and respawn a new one instead. logger [optional - programatically only] - One can supply a custom logger to pipe all the supervisor logs to this logger.","title":"Supervisor"},{"location":"supervisor/#command-line","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 python3 -m sergeant.supervisor --helpusage: supervisor.py [ -h ] --concurrent-workers CONCURRENT_WORKERS --worker-class WORKER_CLASS --worker-module WORKER_MODULE [ --max-worker-memory-usage MAX_WORKER_MEMORY_USAGE ] Sergeant Supervisor optional arguments: -h, --help show this help message and exit --concurrent-workers CONCURRENT_WORKERS Number of subprocesses to open --worker-class WORKER_CLASS Class name of the worker to spawn --worker-module WORKER_MODULE Module of the worker class --max-worker-memory-usage MAX_WORKER_MEMORY_USAGE Maximum RSS memory usage in bytes of an individual worker. When a worker reaches this value, the supevisor would kill it and respawn another one in place.","title":"Command Line"},{"location":"supervisor/#examples","text":"Assuming a Supervisor with concurrency level of 4 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = crawl_worker \\ --worker-class = Worker \\ --concurrent-workers = 4 graph TD Supervisor --> crawl_worker-1 Supervisor --> crawl_worker-2 Supervisor --> crawl_worker-3 Supervisor --> crawl_worker-4 When a worker reaches its end of life - at the moment it finished max_tasks_per_run tasks, it will exit, and a new worker will be created by the supervisor.","title":"Examples"},{"location":"supervisor/#programatically","text":"It is possilbe to programatically invoke a Supervisor if you would like to document your supervisor parameters or if you would like to attach another logger.","title":"Programatically"},{"location":"supervisor/#examples_1","text":"Pay attention to the worker_module_name parameter. The way python find the module name depends on the command line CWD. Look at benchmark/1_simple_worker/sergeant/supervisor.py to see how to work with module names if the supervisor.py file lays aside the worker module. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import sergeant def main (): supervisor = sergeant . supervisor . Supervisor ( worker_module_name = 'consumer' , worker_class_name = 'Worker' , concurrent_workers = 1 , max_worker_memory_usage = None , ) supervisor . start () if __name__ == '__main__' : main ()","title":"Examples"},{"location":"examples/base_worker/","text":"Base Worker # This example demonstrates how to create a base worker so other workers can inherit from and define their own configuration. Code # base.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import sergeant class BaseWorker ( sergeant . worker . Worker , ): config = sergeant . config . WorkerConfig ( name = 'base_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), ) derived_worker.py 1 2 3 4 5 6 7 8 9 10 11 import sergeant from . import base class Worker ( base . BaseWorker , ): config = base . BaseWorker . config . replace ( name = 'derived_worker' , ) Explanation # In order to create a base worker class and implement workers based on it, one should define the base config in the base class object, and use a method replace to copy it with differrent configuration. replace method was implemented using dataclasses.replace method, to copy the dataclass with differrent parameters.","title":"Base Worker"},{"location":"examples/base_worker/#base-worker","text":"This example demonstrates how to create a base worker so other workers can inherit from and define their own configuration.","title":"Base Worker"},{"location":"examples/base_worker/#code","text":"base.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import sergeant class BaseWorker ( sergeant . worker . Worker , ): config = sergeant . config . WorkerConfig ( name = 'base_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), ) derived_worker.py 1 2 3 4 5 6 7 8 9 10 11 import sergeant from . import base class Worker ( base . BaseWorker , ): config = base . BaseWorker . config . replace ( name = 'derived_worker' , )","title":"Code"},{"location":"examples/base_worker/#explanation","text":"In order to create a base worker class and implement workers based on it, one should define the base config in the base class object, and use a method replace to copy it with differrent configuration. replace method was implemented using dataclasses.replace method, to copy the dataclass with differrent parameters.","title":"Explanation"},{"location":"examples/single_producer_consumer/","text":"Single Producer-Consumer # We will start with a simple Consumer-Producer pattern to understand how it feels to work with sergeant Graph # graph LR Producer -.->|Push Tasks| Broker{{Broker}} Broker -.->|Pull Tasks| Consumer Code # consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import sergeant import logging class Worker ( sergeant . worker . Worker , ): config = sergeant . config . WorkerConfig ( name = 'test_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), logging = sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ), ) def work ( self , task , ): self . logger . info ( f 'task parameters are: { task . kwargs } ' ) producer.py 1 2 3 4 5 6 7 8 9 10 11 import consumer worker = consumer . Worker () worker . init_broker () worker . purge_tasks () worker . push_task ( kwargs = { 'some_parameter' : 'one' , }, ) Explanation # Consumer # At the class definition, we inherit from sergeant.worker.Worker to gain all the Worker class abilities. In order for this worker to be able to produce and consume tasks, we need to define config attribute and work method. Defining the config class attribute, allow us to config our worker's abilities. The worker config is a dataclass named sergeant.config.WorkerConfig which has multiple child dataclasses. There are two mandatory fields: name which defines the worker's name, and the queue name within the broker, and the second is the connector . The connector is the device that is responsible to talk with the broker. In this example we define a test_worker worker with a redis connector. logging helps to configure the logger. For each consumed task, the work method is invoked. The parameters are passed inside the task argument as the key kwargs . Producer # The producer first loads the consumer module. The reason for that is that once instantiating a Worker , you can use its configuration. The producer uses the Worker instance so it will have a connection to the broker. Later, we call to init_broker so we will create the connection to the task queue inside the Worker instance. We call purge_tasks to assure no leftover tasks exist in the queue. push_task is the function that compose a task object, and pushes it to the queue. Execution # Producer 1 python3 producer.py Consumer 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = consumer \\ --worker-class = Worker \\ --concurrent-worker = 1","title":"Single Producer-Consumer"},{"location":"examples/single_producer_consumer/#single-producer-consumer","text":"We will start with a simple Consumer-Producer pattern to understand how it feels to work with sergeant","title":"Single Producer-Consumer"},{"location":"examples/single_producer_consumer/#graph","text":"graph LR Producer -.->|Push Tasks| Broker{{Broker}} Broker -.->|Pull Tasks| Consumer","title":"Graph"},{"location":"examples/single_producer_consumer/#code","text":"consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import sergeant import logging class Worker ( sergeant . worker . Worker , ): config = sergeant . config . WorkerConfig ( name = 'test_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), logging = sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ), ) def work ( self , task , ): self . logger . info ( f 'task parameters are: { task . kwargs } ' ) producer.py 1 2 3 4 5 6 7 8 9 10 11 import consumer worker = consumer . Worker () worker . init_broker () worker . purge_tasks () worker . push_task ( kwargs = { 'some_parameter' : 'one' , }, )","title":"Code"},{"location":"examples/single_producer_consumer/#explanation","text":"","title":"Explanation"},{"location":"examples/single_producer_consumer/#consumer","text":"At the class definition, we inherit from sergeant.worker.Worker to gain all the Worker class abilities. In order for this worker to be able to produce and consume tasks, we need to define config attribute and work method. Defining the config class attribute, allow us to config our worker's abilities. The worker config is a dataclass named sergeant.config.WorkerConfig which has multiple child dataclasses. There are two mandatory fields: name which defines the worker's name, and the queue name within the broker, and the second is the connector . The connector is the device that is responsible to talk with the broker. In this example we define a test_worker worker with a redis connector. logging helps to configure the logger. For each consumed task, the work method is invoked. The parameters are passed inside the task argument as the key kwargs .","title":"Consumer"},{"location":"examples/single_producer_consumer/#producer","text":"The producer first loads the consumer module. The reason for that is that once instantiating a Worker , you can use its configuration. The producer uses the Worker instance so it will have a connection to the broker. Later, we call to init_broker so we will create the connection to the task queue inside the Worker instance. We call purge_tasks to assure no leftover tasks exist in the queue. push_task is the function that compose a task object, and pushes it to the queue.","title":"Producer"},{"location":"examples/single_producer_consumer/#execution","text":"Producer 1 python3 producer.py Consumer 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = consumer \\ --worker-class = Worker \\ --concurrent-worker = 1","title":"Execution"},{"location":"examples/supervisor_with_custom_logger/","text":"Supervisor With Custom Logger # This example demonstrates how to create a supervisor module with a custom logger. Code # Both files should be in the same directory to work consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import sergeant class BaseWorker ( sergeant . worker . Worker , ): config = sergeant . config . WorkerConfig ( name = 'some_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), ) ... supervisor.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import sergeant import logging def main (): parent_package_path = '' if '.' in __loader__ . name : parent_package_path = __loader__ . name . rsplit ( '.' , 1 )[ 0 ] if __loader__ . name == '__main__' : parent_package_path = os . path . dirname ( __loader__ . path ) . replace ( '/' , '.' ) . replace ( ' \\\\ ' , '.' ) logger = logging . getLogger ( name = 'Supervisor' , ) logger . addHandler ( sergeant . logging . logstash . LogstashHandler ( host = 'localhost' , port = 9999 , ), ) logger . setLevel ( level = logging . INFO , ) supervisor = sergeant . supervisor . Supervisor ( worker_module_name = f ' { parent_package_path } .consumer' if parent_package_path else 'consumer' , worker_class_name = 'Worker' , concurrent_workers = 1 , max_worker_memory_usage = None , logger = logger , ) supervisor . start () if __name__ == '__main__' : main ()","title":"Supervisor With Custom Logger"},{"location":"examples/supervisor_with_custom_logger/#supervisor-with-custom-logger","text":"This example demonstrates how to create a supervisor module with a custom logger.","title":"Supervisor With Custom Logger"},{"location":"examples/supervisor_with_custom_logger/#code","text":"Both files should be in the same directory to work consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import sergeant class BaseWorker ( sergeant . worker . Worker , ): config = sergeant . config . WorkerConfig ( name = 'some_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), ) ... supervisor.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import sergeant import logging def main (): parent_package_path = '' if '.' in __loader__ . name : parent_package_path = __loader__ . name . rsplit ( '.' , 1 )[ 0 ] if __loader__ . name == '__main__' : parent_package_path = os . path . dirname ( __loader__ . path ) . replace ( '/' , '.' ) . replace ( ' \\\\ ' , '.' ) logger = logging . getLogger ( name = 'Supervisor' , ) logger . addHandler ( sergeant . logging . logstash . LogstashHandler ( host = 'localhost' , port = 9999 , ), ) logger . setLevel ( level = logging . INFO , ) supervisor = sergeant . supervisor . Supervisor ( worker_module_name = f ' { parent_package_path } .consumer' if parent_package_path else 'consumer' , worker_class_name = 'Worker' , concurrent_workers = 1 , max_worker_memory_usage = None , logger = logger , ) supervisor . start () if __name__ == '__main__' : main ()","title":"Code"},{"location":"examples/worker_with_apm/","text":"Worker With APM - ElasticAPM # This example demonstrates how to integrate with an APM solution. In this case, ElasticAPM . Code # consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 import elasticapm import sergeant import logging import requests class Worker ( sergeant . worker . Worker , ): config = sergeant . config . WorkerConfig ( name = 'test_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), max_tasks_per_run = 100 , tasks_per_transaction = 1 , max_retries = 3 , logging = sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ), ) def initialize ( self , ): self . apm_client = elasticapm . Client ( server_url = 'http://localhost:8200/' , environment = 'development' , service_name = self . config . name , service_version = '1.0' , auto_log_stacks = True , collect_local_variables = 'errors' , instrument = True , metrics_interval = '30s' , ) def finalize ( self , ): self . apm_client . close () def pre_work ( self , task , ): self . apm_client . begin_transaction ( transaction_type = 'work' , ) def post_work ( self , task , success , exception , ): if exception is not None : self . apm_client . capture_exception () self . apm_client . end_transaction ( name = 'work' , result = 'success' if success else 'failure' , ) def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url = url_to_crawl , ) response . raise_for_status () producer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from . import consumer def main (): # Init a worker instance to interact with its API worker = consumer . Worker () # Init the worker task queue so we would be able to push tasks to the broker worker . init_broker () # Make sure the queue is empty worker . purge_tasks () # Produce tasks for i in range ( 100 ): worker . push_task ( kwargs = { 'url' : 'https://www.intsights.com/' , }, ) if __name__ == '__main__' : main () Explanation # In order this integrate an APM solution, we implemented initialize , finalize , pre_work and post_work . initialize - Since this function will run once per the whole worker lifetime, the initialization should happen here. This is where we declare the elasticapm.Client . pre_work - This function will run once per task, prior to its execution. This is where we will start a transaction. post_work - This function will run once per task, after its execution. This is where we will end the transaction. This is also where we will try to capture the exception, if any. finalize - This function will run once per the whole worker lifetime. This is where we will do cleanups. In this case, we will implicitly clean the apm client with a call to close . It is important to mention that this example can be easily extended to any other APM solutions such as jaeger and more. Execution # Producer 1 python3 producer.py Consumer 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = consumer \\ --worker-class = Worker \\ --concurrent-worker = 1","title":"Worker With APM - ElasticAPM"},{"location":"examples/worker_with_apm/#worker-with-apm-elasticapm","text":"This example demonstrates how to integrate with an APM solution. In this case, ElasticAPM .","title":"Worker With APM - ElasticAPM"},{"location":"examples/worker_with_apm/#code","text":"consumer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 import elasticapm import sergeant import logging import requests class Worker ( sergeant . worker . Worker , ): config = sergeant . config . WorkerConfig ( name = 'test_worker' , connector = sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ), max_tasks_per_run = 100 , tasks_per_transaction = 1 , max_retries = 3 , logging = sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ), ) def initialize ( self , ): self . apm_client = elasticapm . Client ( server_url = 'http://localhost:8200/' , environment = 'development' , service_name = self . config . name , service_version = '1.0' , auto_log_stacks = True , collect_local_variables = 'errors' , instrument = True , metrics_interval = '30s' , ) def finalize ( self , ): self . apm_client . close () def pre_work ( self , task , ): self . apm_client . begin_transaction ( transaction_type = 'work' , ) def post_work ( self , task , success , exception , ): if exception is not None : self . apm_client . capture_exception () self . apm_client . end_transaction ( name = 'work' , result = 'success' if success else 'failure' , ) def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url = url_to_crawl , ) response . raise_for_status () producer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from . import consumer def main (): # Init a worker instance to interact with its API worker = consumer . Worker () # Init the worker task queue so we would be able to push tasks to the broker worker . init_broker () # Make sure the queue is empty worker . purge_tasks () # Produce tasks for i in range ( 100 ): worker . push_task ( kwargs = { 'url' : 'https://www.intsights.com/' , }, ) if __name__ == '__main__' : main ()","title":"Code"},{"location":"examples/worker_with_apm/#explanation","text":"In order this integrate an APM solution, we implemented initialize , finalize , pre_work and post_work . initialize - Since this function will run once per the whole worker lifetime, the initialization should happen here. This is where we declare the elasticapm.Client . pre_work - This function will run once per task, prior to its execution. This is where we will start a transaction. post_work - This function will run once per task, after its execution. This is where we will end the transaction. This is also where we will try to capture the exception, if any. finalize - This function will run once per the whole worker lifetime. This is where we will do cleanups. In this case, we will implicitly clean the apm client with a call to close . It is important to mention that this example can be easily extended to any other APM solutions such as jaeger and more.","title":"Explanation"},{"location":"examples/worker_with_apm/#execution","text":"Producer 1 python3 producer.py Consumer 1 2 3 4 python3 -m sergeant.supervisor \\ --worker-module = consumer \\ --worker-class = Worker \\ --concurrent-worker = 1","title":"Execution"},{"location":"worker/config/connector/","text":"Worker Config - connector # The connector parameter configures the broker's connection. It is important to note that the broker does not guarantee tasks order. Two consecutive tasks can be pushed to the queue and consumed in a different order. If order does matter to your application, use only one instance of a broker, either redis or mongo based, and the order would stay consistent. Definition # 1 2 3 4 5 6 @dataclasses . dataclass ( frozen = True , ) class Connector : type : str params : typing . Dict [ str , typing . Any ] The type parameter defines the type of the connector. The library currently supports the following types: redis - A Single/Multiple redis instances that are not cluster-connected. The library manages the distribution of tasks on the client side by shuffling the list of connections and push/pull from each of them at different times. Order of tasks is not guaranteed. mongo - A Single/Multiple MongoDB instances that are not cluster-connected. Each of the servers must be configured as a replica set. The library would take care of the replica-set instantiation. This is a great option for persistent tasks. The params parameter is being passed to the connector directly as **kwargs . Examples # redis-single 1 2 3 4 5 6 7 8 9 10 11 12 13 sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ) redis-multi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, { 'host' : 'localhost' , 'port' : 6380 , 'password' : None , 'database' : 0 , }, ], }, ) mongo-single 1 2 3 4 5 6 7 8 9 10 11 12 sergeant . config . Connector ( type = 'mongo' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 27017 , 'replica_set' : 'replica_set_name' , }, ], }, ) mongo-multi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 sergeant . config . Connector ( type = 'mongo' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 27017 , 'replica_set' : 'replica_set_name' , }, { 'host' : 'localhost' , 'port' : 27018 , 'replica_set' : 'replica_set_name' , }, ], }, )","title":"connector"},{"location":"worker/config/connector/#worker-config-connector","text":"The connector parameter configures the broker's connection. It is important to note that the broker does not guarantee tasks order. Two consecutive tasks can be pushed to the queue and consumed in a different order. If order does matter to your application, use only one instance of a broker, either redis or mongo based, and the order would stay consistent.","title":"Worker Config - connector"},{"location":"worker/config/connector/#definition","text":"1 2 3 4 5 6 @dataclasses . dataclass ( frozen = True , ) class Connector : type : str params : typing . Dict [ str , typing . Any ] The type parameter defines the type of the connector. The library currently supports the following types: redis - A Single/Multiple redis instances that are not cluster-connected. The library manages the distribution of tasks on the client side by shuffling the list of connections and push/pull from each of them at different times. Order of tasks is not guaranteed. mongo - A Single/Multiple MongoDB instances that are not cluster-connected. Each of the servers must be configured as a replica set. The library would take care of the replica-set instantiation. This is a great option for persistent tasks. The params parameter is being passed to the connector directly as **kwargs .","title":"Definition"},{"location":"worker/config/connector/#examples","text":"redis-single 1 2 3 4 5 6 7 8 9 10 11 12 13 sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, ], }, ) redis-multi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 sergeant . config . Connector ( type = 'redis' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 6379 , 'password' : None , 'database' : 0 , }, { 'host' : 'localhost' , 'port' : 6380 , 'password' : None , 'database' : 0 , }, ], }, ) mongo-single 1 2 3 4 5 6 7 8 9 10 11 12 sergeant . config . Connector ( type = 'mongo' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 27017 , 'replica_set' : 'replica_set_name' , }, ], }, ) mongo-multi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 sergeant . config . Connector ( type = 'mongo' , params = { 'nodes' : [ { 'host' : 'localhost' , 'port' : 27017 , 'replica_set' : 'replica_set_name' , }, { 'host' : 'localhost' , 'port' : 27018 , 'replica_set' : 'replica_set_name' , }, ], }, )","title":"Examples"},{"location":"worker/config/encoder/","text":"Worker Config - encoder # The encoder parameter controls the encoder which is responsible for the tasks compression and serialization. Definition # 1 2 3 4 5 6 @dataclasses . dataclass ( frozen = True , ) class Encoder : compressor : typing . Optional [ str ] = None serializer : str = 'pickle' The compressor parameter defines the type of the compressor. Each task prior to being pushed to the queue is going through a compressor. The usage of compressor can help reducing the storage/memory of the broker. Tasks with a lot of parameters/data can take a lot of memory and might fill the memory/storage quickly. Using a compressor would impact the performance of task pushing/pulling due to the compression algorithm being a CPU intensive operation. The following compressors are available: None [default] - No compression is applied bzip2 gzip lzma zlib The serializer parameter defines the type of the serializer. Each task prior to being pushed to the queue should be serialized so the broker could save it as a byte array. Choosing the right serialization algorithm is important due to some limitations of each serialization algorithm. The following serializers are available: pickle [default]: pros: fast, native, many supported data types. cons: insecure (allows to run arbitrary code), non-portable, might mislead to think that the parameters were serialized correctly but deserializing them would end with a broken object. msgpack pros: fast, portable, secured. cons: fewer supported data types. One can make any combination of compressor and serializer that suit your needs. Examples # default 1 2 3 4 sergeant . config . Encoder ( compressor = None , serializer = 'pickle' , ) zlib-pickle 1 2 3 4 sergeant . config . Encoder ( compressor = 'zlib' , serializer = 'pickle' , ) lzma-msgpack 1 2 3 4 sergeant . config . Encoder ( compressor = 'lzma' , serializer = 'msgpack' , )","title":"encoder"},{"location":"worker/config/encoder/#worker-config-encoder","text":"The encoder parameter controls the encoder which is responsible for the tasks compression and serialization.","title":"Worker Config - encoder"},{"location":"worker/config/encoder/#definition","text":"1 2 3 4 5 6 @dataclasses . dataclass ( frozen = True , ) class Encoder : compressor : typing . Optional [ str ] = None serializer : str = 'pickle' The compressor parameter defines the type of the compressor. Each task prior to being pushed to the queue is going through a compressor. The usage of compressor can help reducing the storage/memory of the broker. Tasks with a lot of parameters/data can take a lot of memory and might fill the memory/storage quickly. Using a compressor would impact the performance of task pushing/pulling due to the compression algorithm being a CPU intensive operation. The following compressors are available: None [default] - No compression is applied bzip2 gzip lzma zlib The serializer parameter defines the type of the serializer. Each task prior to being pushed to the queue should be serialized so the broker could save it as a byte array. Choosing the right serialization algorithm is important due to some limitations of each serialization algorithm. The following serializers are available: pickle [default]: pros: fast, native, many supported data types. cons: insecure (allows to run arbitrary code), non-portable, might mislead to think that the parameters were serialized correctly but deserializing them would end with a broken object. msgpack pros: fast, portable, secured. cons: fewer supported data types. One can make any combination of compressor and serializer that suit your needs.","title":"Definition"},{"location":"worker/config/encoder/#examples","text":"default 1 2 3 4 sergeant . config . Encoder ( compressor = None , serializer = 'pickle' , ) zlib-pickle 1 2 3 4 sergeant . config . Encoder ( compressor = 'zlib' , serializer = 'pickle' , ) lzma-msgpack 1 2 3 4 sergeant . config . Encoder ( compressor = 'lzma' , serializer = 'msgpack' , )","title":"Examples"},{"location":"worker/config/logging/","text":"Worker Config - timeouts # The logging parameter controls the logger of the worker. Definition # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @dataclasses . dataclass ( frozen = True , ) class LoggingEvents : on_success : bool = False on_failure : bool = True on_timeout : bool = True on_retry : bool = True on_max_retries : bool = True on_requeue : bool = True @dataclasses . dataclass ( frozen = True , ) class Logging : level : int = logging . ERROR log_to_stdout : bool = False events : LoggingEvents = dataclasses . field ( default_factory = LoggingEvents , ) handlers : typing . List [ logging . Handler ] = dataclasses . field ( default_factory = list , ) The following configurations are available: level [logging.ERROR] - The logging.level of the logger. Can be one of the available levels. log_to_stdout [False] - Whether the logger should log to stdout. events - On which events the logger should log. on_success [False] - Every time a task has finished successfully. on_failure [True] - Every time a task has failed. on_timeout [True] - Every time a task timed out. on_retry [True] - Every time a task asked for a retry. on_max_retries [True] - Every time a task asked for a retry beyond the maximum number of retries. on_requeue [True] - Every time a task asked for a requeue. handlers - List of handlers [logging.Handler] to attach to the logging.Logger object. Examples # STDOUT 1 2 3 4 sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ) Logstash 1 2 3 4 5 6 7 8 9 10 sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , handlers = [ sergeant . logging . logstash . LogstashHandler ( host = 'localhost' , port = 9999 , ), ], )","title":"logging"},{"location":"worker/config/logging/#worker-config-timeouts","text":"The logging parameter controls the logger of the worker.","title":"Worker Config - timeouts"},{"location":"worker/config/logging/#definition","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @dataclasses . dataclass ( frozen = True , ) class LoggingEvents : on_success : bool = False on_failure : bool = True on_timeout : bool = True on_retry : bool = True on_max_retries : bool = True on_requeue : bool = True @dataclasses . dataclass ( frozen = True , ) class Logging : level : int = logging . ERROR log_to_stdout : bool = False events : LoggingEvents = dataclasses . field ( default_factory = LoggingEvents , ) handlers : typing . List [ logging . Handler ] = dataclasses . field ( default_factory = list , ) The following configurations are available: level [logging.ERROR] - The logging.level of the logger. Can be one of the available levels. log_to_stdout [False] - Whether the logger should log to stdout. events - On which events the logger should log. on_success [False] - Every time a task has finished successfully. on_failure [True] - Every time a task has failed. on_timeout [True] - Every time a task timed out. on_retry [True] - Every time a task asked for a retry. on_max_retries [True] - Every time a task asked for a retry beyond the maximum number of retries. on_requeue [True] - Every time a task asked for a requeue. handlers - List of handlers [logging.Handler] to attach to the logging.Logger object.","title":"Definition"},{"location":"worker/config/logging/#examples","text":"STDOUT 1 2 3 4 sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , ) Logstash 1 2 3 4 5 6 7 8 9 10 sergeant . config . Logging ( level = logging . INFO , log_to_stdout = True , handlers = [ sergeant . logging . logstash . LogstashHandler ( host = 'localhost' , port = 9999 , ), ], )","title":"Examples"},{"location":"worker/config/max_retries/","text":"Worker Config - max_retries # max_retries defines how many retries the worker can invoke before the task will be dumped. This number should be int > 0 . Using this parameter is encouraged for tasks that retry exist within them. retry method is a function that sends back the current task to the queue after increasing the internal run_count . When the run_count reaches the max_retries number, calling retry again would result in an exception. Tasks that you don't want to retry them forever might configure this parameter. A value of 0 means you can retry infinitely. Definition # 1 max_retries : int = 0","title":"max_retries"},{"location":"worker/config/max_retries/#worker-config-max_retries","text":"max_retries defines how many retries the worker can invoke before the task will be dumped. This number should be int > 0 . Using this parameter is encouraged for tasks that retry exist within them. retry method is a function that sends back the current task to the queue after increasing the internal run_count . When the run_count reaches the max_retries number, calling retry again would result in an exception. Tasks that you don't want to retry them forever might configure this parameter. A value of 0 means you can retry infinitely.","title":"Worker Config - max_retries"},{"location":"worker/config/max_retries/#definition","text":"1 max_retries : int = 0","title":"Definition"},{"location":"worker/config/max_tasks_per_run/","text":"Worker Config - max_tasks_per_run # max_tasks_per_run defines how many tasks the worker should consume before killing it self and spawning another worker instead. This number should be int > 0 . The usage of this parameter is encouraged even if you think your worker must respawn itself ever. Memory leak situations might occuer from time to time without any simptoms. Declaring this parameter, might keep your worker healthy over time. A low number here is discouraged unless neccessary. Low number might cause the worker to die frequently and the overhead of respawning might be felt. A value of 0 means worker should never die. Definition # 1 max_tasks_per_run : int = 0","title":"max_tasks_per_run"},{"location":"worker/config/max_tasks_per_run/#worker-config-max_tasks_per_run","text":"max_tasks_per_run defines how many tasks the worker should consume before killing it self and spawning another worker instead. This number should be int > 0 . The usage of this parameter is encouraged even if you think your worker must respawn itself ever. Memory leak situations might occuer from time to time without any simptoms. Declaring this parameter, might keep your worker healthy over time. A low number here is discouraged unless neccessary. Low number might cause the worker to die frequently and the overhead of respawning might be felt. A value of 0 means worker should never die.","title":"Worker Config - max_tasks_per_run"},{"location":"worker/config/max_tasks_per_run/#definition","text":"1 max_tasks_per_run : int = 0","title":"Definition"},{"location":"worker/config/name/","text":"Worker Config - name # The name parameter an is important parameter inside the architecture. This parameter is mandatory. The usage of this parameter is crucial for the worker to be able to consume tasks. This parameter eventually becomes the queue name inside the broker. The producer should usu that name to invoke the corresponding worker. Using the same worker name with the same broker, would make the two workers share the same queue and eventually consume each other tasks. Definition # 1 name : str","title":"name"},{"location":"worker/config/name/#worker-config-name","text":"The name parameter an is important parameter inside the architecture. This parameter is mandatory. The usage of this parameter is crucial for the worker to be able to consume tasks. This parameter eventually becomes the queue name inside the broker. The producer should usu that name to invoke the corresponding worker. Using the same worker name with the same broker, would make the two workers share the same queue and eventually consume each other tasks.","title":"Worker Config - name"},{"location":"worker/config/name/#definition","text":"1 name : str","title":"Definition"},{"location":"worker/config/number_of_threads/","text":"Worker Config - number_of_threads # The number_of_threads parameter controls how many thread to use for the tasks execution. Under the hood, it controls which executor to use when executing tasks. Definition # 1 number_of_threads : int = 1 The number_of_threads parameter defines the number of thread to use when executing tasks. When the number of threads is 1, the serial executor would be in use, otherwise the threaded executor would be in use. serial [default] - Serial executor takes the bulk of the tasks that were pulled from the broker, and executes each one of them one by one. This executor means there is no parallelism within the process. threaded - Threaded executor takes the bulk of the tasks that were pulled from the broker, and executes them within a thread pool. This executor means that on the top of the process, there is parallelism between different threads. It is very important to choose the right executor type. There are more consequences than one might think for choosing the incorrect executor. When using the serial executor, the only concurrency you will get will be the amount of workers that were spawned by the supervisor . It is a very stable executor due to its nature. It takes a task after task, and runs it in a loop. The threaded executor take a bunch of tasks and runs them inside a thread pool. The big concern here is the inability to stop an execution of a thread in Python. A thread that would lock the GIL would make the worker inaccesible. The worker would be in a stuck mode. There are two different killers for each of the executors. When using the serial executor, the sergeant.killer.process is being used. This type of killer can kill the process entirely on situations where the process became stuck. When using the threaded executor, the sergeant.killer.thread is being used. This type of killer tries to raise an exception inside the stuck thread. If the thread holds the GIL, or is not switching its current execution line, it would stuck forever. A call for time.sleep(100) for example would not raise an exception until the sleep would end. The reason not to use signals here as we do in sergeant.killer.process is that we do not want to interrupt all the threads, and this would cause to drop multiple tasks.","title":"number_of_threads"},{"location":"worker/config/number_of_threads/#worker-config-number_of_threads","text":"The number_of_threads parameter controls how many thread to use for the tasks execution. Under the hood, it controls which executor to use when executing tasks.","title":"Worker Config - number_of_threads"},{"location":"worker/config/number_of_threads/#definition","text":"1 number_of_threads : int = 1 The number_of_threads parameter defines the number of thread to use when executing tasks. When the number of threads is 1, the serial executor would be in use, otherwise the threaded executor would be in use. serial [default] - Serial executor takes the bulk of the tasks that were pulled from the broker, and executes each one of them one by one. This executor means there is no parallelism within the process. threaded - Threaded executor takes the bulk of the tasks that were pulled from the broker, and executes them within a thread pool. This executor means that on the top of the process, there is parallelism between different threads. It is very important to choose the right executor type. There are more consequences than one might think for choosing the incorrect executor. When using the serial executor, the only concurrency you will get will be the amount of workers that were spawned by the supervisor . It is a very stable executor due to its nature. It takes a task after task, and runs it in a loop. The threaded executor take a bunch of tasks and runs them inside a thread pool. The big concern here is the inability to stop an execution of a thread in Python. A thread that would lock the GIL would make the worker inaccesible. The worker would be in a stuck mode. There are two different killers for each of the executors. When using the serial executor, the sergeant.killer.process is being used. This type of killer can kill the process entirely on situations where the process became stuck. When using the threaded executor, the sergeant.killer.thread is being used. This type of killer tries to raise an exception inside the stuck thread. If the thread holds the GIL, or is not switching its current execution line, it would stuck forever. A call for time.sleep(100) for example would not raise an exception until the sleep would end. The reason not to use signals here as we do in sergeant.killer.process is that we do not want to interrupt all the threads, and this would cause to drop multiple tasks.","title":"Definition"},{"location":"worker/config/starvation/","text":"Worker Config - starvation # The starvation parameter controls the starvation logic of the worker. Starvation is considered as a situation where the worker is trying to pull tasks from the queue without success. It can happen in multiple situations. First, it can happen when there are no tasks left in the queue. Second, it can happen when other workers are faster in pulling tasks from the queue and the current worker is redundant. Third, it can happen when the worker has connectivity issues. It can probably happen in more cases. The idea of starvation is to detect when the worker is running when it can do nothing regarding its purpose. Definition # 1 2 3 4 5 @dataclasses . dataclass ( frozen = True , ) class Starvation : time_with_no_tasks : int The following configurations are available: time_with_no_tasks - How many seconds without being able to pull tasks from the queue will be considered as a starvation.","title":"starvation"},{"location":"worker/config/starvation/#worker-config-starvation","text":"The starvation parameter controls the starvation logic of the worker. Starvation is considered as a situation where the worker is trying to pull tasks from the queue without success. It can happen in multiple situations. First, it can happen when there are no tasks left in the queue. Second, it can happen when other workers are faster in pulling tasks from the queue and the current worker is redundant. Third, it can happen when the worker has connectivity issues. It can probably happen in more cases. The idea of starvation is to detect when the worker is running when it can do nothing regarding its purpose.","title":"Worker Config - starvation"},{"location":"worker/config/starvation/#definition","text":"1 2 3 4 5 @dataclasses . dataclass ( frozen = True , ) class Starvation : time_with_no_tasks : int The following configurations are available: time_with_no_tasks - How many seconds without being able to pull tasks from the queue will be considered as a starvation.","title":"Definition"},{"location":"worker/config/tasks_per_transaction/","text":"Worker Config - tasks_per_transaction # tasks_per_transaction defines how many tasks the worker will pull from the broker on each transaction. The worker loop works as follow: Worker pulls tasks from the broker. The worker tries to pull tasks_per_transaction tasks in one command. The worker executes each of the tasks one after one. At the end of executing all the pulled tasks, repeat phase 1. The reason the worker works in this way is to reduce the load on the broker. Pulling a bulk of 100 tasks is much easier than pulling a single task 100 times. The reason not to do that is to allow better distributions of tasks. Think about the situation that you have a task, that its execution takes 1 minute. Pulling 100 tasks would repeat every 100 minutes. Imagine running 2 workers to consume this task queue, and a producer that pushed only 100 tasks. The first worker to pull from the broker would consume all the tasks and leave nothing to the other worker. Think about another situation where you have a task that might take 1 second to 1 minute to complete. The producer would push for example 100 tasks, each worker would pull 50 tasks. The first worker finished its tasks within 1 minute for example and the second worker randomlly pulled longer to execute tasks. The first worker would starve for tasks while the other exhaust. This parameter should be chosen wisely. When you have a lot of tasks and each task is short, feel free to raise the number of tasks per transaction. When you have long tasks, leave this number as 1 to allow better tasks distribution. Definition # 1 tasks_per_transaction : int = 1","title":"tasks_per_transaction"},{"location":"worker/config/tasks_per_transaction/#worker-config-tasks_per_transaction","text":"tasks_per_transaction defines how many tasks the worker will pull from the broker on each transaction. The worker loop works as follow: Worker pulls tasks from the broker. The worker tries to pull tasks_per_transaction tasks in one command. The worker executes each of the tasks one after one. At the end of executing all the pulled tasks, repeat phase 1. The reason the worker works in this way is to reduce the load on the broker. Pulling a bulk of 100 tasks is much easier than pulling a single task 100 times. The reason not to do that is to allow better distributions of tasks. Think about the situation that you have a task, that its execution takes 1 minute. Pulling 100 tasks would repeat every 100 minutes. Imagine running 2 workers to consume this task queue, and a producer that pushed only 100 tasks. The first worker to pull from the broker would consume all the tasks and leave nothing to the other worker. Think about another situation where you have a task that might take 1 second to 1 minute to complete. The producer would push for example 100 tasks, each worker would pull 50 tasks. The first worker finished its tasks within 1 minute for example and the second worker randomlly pulled longer to execute tasks. The first worker would starve for tasks while the other exhaust. This parameter should be chosen wisely. When you have a lot of tasks and each task is short, feel free to raise the number of tasks per transaction. When you have long tasks, leave this number as 1 to allow better tasks distribution.","title":"Worker Config - tasks_per_transaction"},{"location":"worker/config/tasks_per_transaction/#definition","text":"1 tasks_per_transaction : int = 1","title":"Definition"},{"location":"worker/config/timeouts/","text":"Worker Config - timeouts # The timeouts parameter controls the killer timeouts for the worker. Definition # 1 2 3 4 5 6 @dataclasses . dataclass ( frozen = True , ) class Timeouts : timeout : float = 0.0 grace_period : float = 10.0 The timeouts parameter defines how much time the process should run before the killer must try to kill it. timeout - On serial executor, by the time this timeout is reached, a SIGTERM is sent to the worker. On threaded worker, an exception would be raised inside the thread. grace_period [serial] - If the worker will not become responding after the SIGTERM is sent, it will be escalated to a SIGKILL signal. By default, no timeouts are applied. It means that the tasks will never timeout. One should use timeouts wisely and set them according to the expected type of the task. If the task, in its ordinary case, should run for no longer than 30s, you can set the timeout to 1m and keep the task from being stuck forever. Examples # 1 2 3 sergeant . config . Timeouts ( timeout = 10.0 , )","title":"timeouts"},{"location":"worker/config/timeouts/#worker-config-timeouts","text":"The timeouts parameter controls the killer timeouts for the worker.","title":"Worker Config - timeouts"},{"location":"worker/config/timeouts/#definition","text":"1 2 3 4 5 6 @dataclasses . dataclass ( frozen = True , ) class Timeouts : timeout : float = 0.0 grace_period : float = 10.0 The timeouts parameter defines how much time the process should run before the killer must try to kill it. timeout - On serial executor, by the time this timeout is reached, a SIGTERM is sent to the worker. On threaded worker, an exception would be raised inside the thread. grace_period [serial] - If the worker will not become responding after the SIGTERM is sent, it will be escalated to a SIGKILL signal. By default, no timeouts are applied. It means that the tasks will never timeout. One should use timeouts wisely and set them according to the expected type of the task. If the task, in its ordinary case, should run for no longer than 30s, you can set the timeout to 1m and keep the task from being stuck forever.","title":"Definition"},{"location":"worker/config/timeouts/#examples","text":"1 2 3 sergeant . config . Timeouts ( timeout = 10.0 , )","title":"Examples"},{"location":"worker/handlers/on_failure/","text":"Worker Handler - on_failure # The on_failure handler is invoked when a task has raised an exception. The exception object will be passed to the handler. Definition # 1 2 3 4 5 def on_failure ( self , task : sergeant . objects . Task , exception : Exception , ) -> None The following use cases are possible: Fire a logging event. Implement a metrics collector. Cleanup task's traces Call retry / requeue to retry on failures","title":"on_failure"},{"location":"worker/handlers/on_failure/#worker-handler-on_failure","text":"The on_failure handler is invoked when a task has raised an exception. The exception object will be passed to the handler.","title":"Worker Handler - on_failure"},{"location":"worker/handlers/on_failure/#definition","text":"1 2 3 4 5 def on_failure ( self , task : sergeant . objects . Task , exception : Exception , ) -> None The following use cases are possible: Fire a logging event. Implement a metrics collector. Cleanup task's traces Call retry / requeue to retry on failures","title":"Definition"},{"location":"worker/handlers/on_max_retries/","text":"Worker Handler - on_max_retries # The on_max_retries handler is invoked when a task called the retry method more than the allowed number of times. Definition # 1 2 3 4 def on_max_retries ( self , task : sergeant . objects . Task , ) -> None The following use cases are possible: Fire a logging event Implement a metrics collector.","title":"on_max_retries"},{"location":"worker/handlers/on_max_retries/#worker-handler-on_max_retries","text":"The on_max_retries handler is invoked when a task called the retry method more than the allowed number of times.","title":"Worker Handler - on_max_retries"},{"location":"worker/handlers/on_max_retries/#definition","text":"1 2 3 4 def on_max_retries ( self , task : sergeant . objects . Task , ) -> None The following use cases are possible: Fire a logging event Implement a metrics collector.","title":"Definition"},{"location":"worker/handlers/on_requeue/","text":"Worker Handler - on_requeue # The on_requeue handler is invoked when a task called the requeue method. Definition # 1 2 3 4 def on_requeue ( self , task : sergeant . objects . Task , ) -> None The following use cases are possible: Fire a logging event Implement a metrics collector.","title":"on_requeue"},{"location":"worker/handlers/on_requeue/#worker-handler-on_requeue","text":"The on_requeue handler is invoked when a task called the requeue method.","title":"Worker Handler - on_requeue"},{"location":"worker/handlers/on_requeue/#definition","text":"1 2 3 4 def on_requeue ( self , task : sergeant . objects . Task , ) -> None The following use cases are possible: Fire a logging event Implement a metrics collector.","title":"Definition"},{"location":"worker/handlers/on_retry/","text":"Worker Handler - on_retry # The on_retry handler is invoked when a task called the retry method. Definition # 1 2 3 4 def on_retry ( self , task : sergeant . objects . Task , ) -> None The following use cases are possible: Fire a logging event Implement a metrics collector.","title":"on_retry"},{"location":"worker/handlers/on_retry/#worker-handler-on_retry","text":"The on_retry handler is invoked when a task called the retry method.","title":"Worker Handler - on_retry"},{"location":"worker/handlers/on_retry/#definition","text":"1 2 3 4 def on_retry ( self , task : sergeant . objects . Task , ) -> None The following use cases are possible: Fire a logging event Implement a metrics collector.","title":"Definition"},{"location":"worker/handlers/on_starvation/","text":"Worker Handler - on_starvation # The on_starvation handler is invoked when a worker becomes starved - meaning it could not pull tasks from the broker because there were no tasks left in the queue. This handler is only invoked when the worker was configured with the starvation field. The field time_with_no_tasks means after how many seconds without tasks the worker should trigger this handler. Definition # 1 2 3 4 5 def on_starvation ( self , time_with_no_tasks : int , ) -> None : pass The following use cases are possible: Fire a logging event. Hint an external autoscaler to reduce the amount of worker in the system. Call stop to kill the worker as it is no longer needed. Call respawn to respawn the worker process as it is an opportunity to release memory and start from scratch.","title":"on_starvation"},{"location":"worker/handlers/on_starvation/#worker-handler-on_starvation","text":"The on_starvation handler is invoked when a worker becomes starved - meaning it could not pull tasks from the broker because there were no tasks left in the queue. This handler is only invoked when the worker was configured with the starvation field. The field time_with_no_tasks means after how many seconds without tasks the worker should trigger this handler.","title":"Worker Handler - on_starvation"},{"location":"worker/handlers/on_starvation/#definition","text":"1 2 3 4 5 def on_starvation ( self , time_with_no_tasks : int , ) -> None : pass The following use cases are possible: Fire a logging event. Hint an external autoscaler to reduce the amount of worker in the system. Call stop to kill the worker as it is no longer needed. Call respawn to respawn the worker process as it is an opportunity to release memory and start from scratch.","title":"Definition"},{"location":"worker/handlers/on_success/","text":"Worker Handler - on_success # The on_success handler is invoked when a task has completed successfully. The returned value will be passed to the handler. Definition # 1 2 3 4 5 def on_success ( self , task : sergeant . objects . Task , returned_value : typing . Any , ) -> None When the task's work method has finished successfully - without any exception being raised, without any retry attempts, this handler will be invoked. The returned value of the task will be passed to the handler. The following use cases are possible: Fire a logging event. Implement a metrics collector.","title":"on_success"},{"location":"worker/handlers/on_success/#worker-handler-on_success","text":"The on_success handler is invoked when a task has completed successfully. The returned value will be passed to the handler.","title":"Worker Handler - on_success"},{"location":"worker/handlers/on_success/#definition","text":"1 2 3 4 5 def on_success ( self , task : sergeant . objects . Task , returned_value : typing . Any , ) -> None When the task's work method has finished successfully - without any exception being raised, without any retry attempts, this handler will be invoked. The returned value of the task will be passed to the handler. The following use cases are possible: Fire a logging event. Implement a metrics collector.","title":"Definition"},{"location":"worker/handlers/on_timeout/","text":"Worker Handler - on_timeout # The on_timeout handler is invoked when a task has timed out. Unlike other events, this event is triggered by the Killer and not by something the process has done. Definition # 1 2 3 4 def on_timeout ( self , task : sergeant . objects . Task , ) -> None The following use cases are possible: Fire a logging event. Implement a metrics collector. Cleanup task's traces Call retry / requeue to retry on timeouts","title":"on_timeout"},{"location":"worker/handlers/on_timeout/#worker-handler-on_timeout","text":"The on_timeout handler is invoked when a task has timed out. Unlike other events, this event is triggered by the Killer and not by something the process has done.","title":"Worker Handler - on_timeout"},{"location":"worker/handlers/on_timeout/#definition","text":"1 2 3 4 def on_timeout ( self , task : sergeant . objects . Task , ) -> None The following use cases are possible: Fire a logging event. Implement a metrics collector. Cleanup task's traces Call retry / requeue to retry on timeouts","title":"Definition"},{"location":"worker/lock/acquire/","text":"Worker - acquire # The acquire method tries to acquire the lock. The method allows to wait for the lock to become available. When the lock is acquired, the return value is True , otherwise it returns False . timeout : How much time (in seconds) to wait until the lock is acquireable. check_interval : How frequently (in seconds) to check the lock. ttl : Once the lock is acquired, how long (in seconds) should be its lifetime. Definition # 1 2 3 4 5 6 def acquire ( self , timeout : typing . Optional [ float ] = None , check_interval : float = 1.0 , ttl : int = 60 , ) -> bool Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) acquired = lock . acquire ( timeout = 10 , check_interval = 1.0 , ttl = 60 , ) if acquired : try : response = requests . get ( single_access_api_url ) finally : lock . release ()","title":"acquire"},{"location":"worker/lock/acquire/#worker-acquire","text":"The acquire method tries to acquire the lock. The method allows to wait for the lock to become available. When the lock is acquired, the return value is True , otherwise it returns False . timeout : How much time (in seconds) to wait until the lock is acquireable. check_interval : How frequently (in seconds) to check the lock. ttl : Once the lock is acquired, how long (in seconds) should be its lifetime.","title":"Worker - acquire"},{"location":"worker/lock/acquire/#definition","text":"1 2 3 4 5 6 def acquire ( self , timeout : typing . Optional [ float ] = None , check_interval : float = 1.0 , ttl : int = 60 , ) -> bool","title":"Definition"},{"location":"worker/lock/acquire/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) acquired = lock . acquire ( timeout = 10 , check_interval = 1.0 , ttl = 60 , ) if acquired : try : response = requests . get ( single_access_api_url ) finally : lock . release ()","title":"Examples"},{"location":"worker/lock/get_ttl/","text":"Worker - get_ttl # The get_ttl method return how much time is seconds left until the lock will expire. An expired lock is not necessarily a lockable lock. There might be other worker that are waiting for the lock to expire. It is an idication thoguh, to decide whether we want to wait or skip to the next task. Return the number of seconds until the lock expires, or None if there is no lock or it has already expired. Definition # 1 2 3 def get_ttl ( self , ) -> typing . Optional [ int ] Examples # Simple 1 2 3 4 5 6 7 8 9 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) if lock . get_ttl () > 10 : self . requeue ()","title":"get_ttl"},{"location":"worker/lock/get_ttl/#worker-get_ttl","text":"The get_ttl method return how much time is seconds left until the lock will expire. An expired lock is not necessarily a lockable lock. There might be other worker that are waiting for the lock to expire. It is an idication thoguh, to decide whether we want to wait or skip to the next task. Return the number of seconds until the lock expires, or None if there is no lock or it has already expired.","title":"Worker - get_ttl"},{"location":"worker/lock/get_ttl/#definition","text":"1 2 3 def get_ttl ( self , ) -> typing . Optional [ int ]","title":"Definition"},{"location":"worker/lock/get_ttl/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) if lock . get_ttl () > 10 : self . requeue ()","title":"Examples"},{"location":"worker/lock/is_locked/","text":"Worker - is_locked # The is_locked method return whether the lock is locked or unlocked. Definition # 1 2 3 def is_locked ( self , ) -> bool Examples # Simple 1 2 3 4 5 6 7 8 9 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) if lock . is_locked (): self . requeue ()","title":"is_locked"},{"location":"worker/lock/is_locked/#worker-is_locked","text":"The is_locked method return whether the lock is locked or unlocked.","title":"Worker - is_locked"},{"location":"worker/lock/is_locked/#definition","text":"1 2 3 def is_locked ( self , ) -> bool","title":"Definition"},{"location":"worker/lock/is_locked/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) if lock . is_locked (): self . requeue ()","title":"Examples"},{"location":"worker/lock/release/","text":"Worker - release # The release method releases the lock and allows other workers to acquire the lock. If the lock was released, the return value is True , otherwise it returns False . Never release unless you were the owner of the lock When some worker calls release on a lock that it has not previously acquired, it will release the lock, and make the lock available again, even though some other worker is dependant on it. Always validate you were the owner of the lock. Definition # 1 2 3 def release ( self , ) -> bool Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) acquired = lock . acquire ( timeout = 10 , check_interval = 1.0 , ttl = 60 , ) if acquired : try : response = requests . get ( single_access_api_url ) finally : lock . release ()","title":"release"},{"location":"worker/lock/release/#worker-release","text":"The release method releases the lock and allows other workers to acquire the lock. If the lock was released, the return value is True , otherwise it returns False . Never release unless you were the owner of the lock When some worker calls release on a lock that it has not previously acquired, it will release the lock, and make the lock available again, even though some other worker is dependant on it. Always validate you were the owner of the lock.","title":"Worker - release"},{"location":"worker/lock/release/#definition","text":"1 2 3 def release ( self , ) -> bool","title":"Definition"},{"location":"worker/lock/release/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) acquired = lock . acquire ( timeout = 10 , check_interval = 1.0 , ttl = 60 , ) if acquired : try : response = requests . get ( single_access_api_url ) finally : lock . release ()","title":"Examples"},{"location":"worker/lock/set_ttl/","text":"Worker - set_ttl # The set_ttl sets the lock ttl (in seconds). If there is no lock with the same name, it will return False , otherwise it will return True . This method could help a worker to extend the lock lifetime. Definition # 1 2 3 4 def set_ttl ( self , ttl : int , ) -> bool Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) if self . acquire ( timeout = 10 , ttl = 20 , ): while True : result = some_api () if result . not_yet_available : lock . set_ttl ( 20 ) else : finalize ( result )","title":"set_ttl"},{"location":"worker/lock/set_ttl/#worker-set_ttl","text":"The set_ttl sets the lock ttl (in seconds). If there is no lock with the same name, it will return False , otherwise it will return True . This method could help a worker to extend the lock lifetime.","title":"Worker - set_ttl"},{"location":"worker/lock/set_ttl/#definition","text":"1 2 3 4 def set_ttl ( self , ttl : int , ) -> bool","title":"Definition"},{"location":"worker/lock/set_ttl/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) if self . acquire ( timeout = 10 , ttl = 20 , ): while True : result = some_api () if result . not_yet_available : lock . set_ttl ( 20 ) else : finalize ( result )","title":"Examples"},{"location":"worker/methods/get_next_tasks/","text":"Worker - get_next_tasks # The get_next_tasks method pulls number_of_tasks tasks from the queue. Unless task_name was specified, uses the current worker name. No one should use this function directly unless they know what they are doing. Definition # 1 2 3 4 5 def get_next_tasks ( self , number_of_tasks : int , task_name : typing . Optional [ str ] = None , ) -> typing . List [ typing . Dict [ str , typing . Any ]] Examples # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def work ( self , task , ): statistics = { 'google.com' : 0 , 'facebook.com' : 0 , } while True : tasks = self . get_next_tasks ( number_of_tasks = 1000 , task_name = 'unfiltered_task' , ) if not tasks : break for task in tasks : domain = task . kwargs [ 'domain' ] if domain not in statistics : continue statistics [ domain ] += 1 self . push_task ( kwargs = { 'domain' : domain , 'params' : task . kwargs [ 'params' ] }, task_name = 'filtered_task' , )","title":"get_next_tasks"},{"location":"worker/methods/get_next_tasks/#worker-get_next_tasks","text":"The get_next_tasks method pulls number_of_tasks tasks from the queue. Unless task_name was specified, uses the current worker name. No one should use this function directly unless they know what they are doing.","title":"Worker - get_next_tasks"},{"location":"worker/methods/get_next_tasks/#definition","text":"1 2 3 4 5 def get_next_tasks ( self , number_of_tasks : int , task_name : typing . Optional [ str ] = None , ) -> typing . List [ typing . Dict [ str , typing . Any ]]","title":"Definition"},{"location":"worker/methods/get_next_tasks/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def work ( self , task , ): statistics = { 'google.com' : 0 , 'facebook.com' : 0 , } while True : tasks = self . get_next_tasks ( number_of_tasks = 1000 , task_name = 'unfiltered_task' , ) if not tasks : break for task in tasks : domain = task . kwargs [ 'domain' ] if domain not in statistics : continue statistics [ domain ] += 1 self . push_task ( kwargs = { 'domain' : domain , 'params' : task . kwargs [ 'params' ] }, task_name = 'filtered_task' , )","title":"Examples"},{"location":"worker/methods/lock/","text":"Worker - lock # The lock method creates a distributed lock in the broker. This way multiple workers can synchronize work with the help of a distributed lock. Many use cases of a distributed lock are available such as lock an access to a throttled resource to avoid exceeding its rate limit. Definition # 1 2 3 4 def lock ( self , name : str , ) -> typing . Union [ connector . mongo . Lock , connector . redis . Lock ]: Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) acquired = lock . acquire ( timeout = 10 , ttl = 60 , ) if acquired : try : response = requests . get ( single_access_api_url ) finally : lock . release ()","title":"lock"},{"location":"worker/methods/lock/#worker-lock","text":"The lock method creates a distributed lock in the broker. This way multiple workers can synchronize work with the help of a distributed lock. Many use cases of a distributed lock are available such as lock an access to a throttled resource to avoid exceeding its rate limit.","title":"Worker - lock"},{"location":"worker/methods/lock/#definition","text":"1 2 3 4 def lock ( self , name : str , ) -> typing . Union [ connector . mongo . Lock , connector . redis . Lock ]:","title":"Definition"},{"location":"worker/methods/lock/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def work ( self , task , ): single_access_api_url = task . kwargs [ 'single_access_api_url' ] lock = self . lock ( 'single_access_api' ) acquired = lock . acquire ( timeout = 10 , ttl = 60 , ) if acquired : try : response = requests . get ( single_access_api_url ) finally : lock . release ()","title":"Examples"},{"location":"worker/methods/number_of_enqueued_tasks/","text":"Worker - number_of_enqueued_tasks # The number_of_enqueued_tasks method return the number of all the tasks from the queue. Unless task_name was specified, uses the current worker name. include_delayed means whether to count delayed tasks too. A good reason to use include_delayed parameter is when you implement an autoscaler, and you want to upscale the number of workers based on the number of consumable tasks rather than the number of tasks along with the delayed tasks. Definition # 1 2 3 4 5 def number_of_enqueued_tasks ( self , task_name : typing . Optional [ str ] = None , include_delayed : bool = False , ) -> typing . Optional [ int ] Examples # 1 2 3 4 5 6 7 def work ( self , task , ): number_of_enqueue_tasks = self . number_of_enqueued_tasks () if number_of_enqueue_tasks > 100000 : self . autoscaler . increase_scaling ()","title":"number_of_enqueued_tasks"},{"location":"worker/methods/number_of_enqueued_tasks/#worker-number_of_enqueued_tasks","text":"The number_of_enqueued_tasks method return the number of all the tasks from the queue. Unless task_name was specified, uses the current worker name. include_delayed means whether to count delayed tasks too. A good reason to use include_delayed parameter is when you implement an autoscaler, and you want to upscale the number of workers based on the number of consumable tasks rather than the number of tasks along with the delayed tasks.","title":"Worker - number_of_enqueued_tasks"},{"location":"worker/methods/number_of_enqueued_tasks/#definition","text":"1 2 3 4 5 def number_of_enqueued_tasks ( self , task_name : typing . Optional [ str ] = None , include_delayed : bool = False , ) -> typing . Optional [ int ]","title":"Definition"},{"location":"worker/methods/number_of_enqueued_tasks/#examples","text":"1 2 3 4 5 6 7 def work ( self , task , ): number_of_enqueue_tasks = self . number_of_enqueued_tasks () if number_of_enqueue_tasks > 100000 : self . autoscaler . increase_scaling ()","title":"Examples"},{"location":"worker/methods/purge_tasks/","text":"Worker - purge_tasks # The purge_tasks method deleted all the tasks from the queue. It allows the worker to implement some critical functionality where it identified a situation that should stop all the future tasks from being executed. Unless task_name was specified, uses the current worker name. Definition # 1 2 3 4 def purge_tasks ( self , task_name : typing . Optional [ str ] = None , ) -> bool Examples # 1 2 3 4 5 6 7 8 9 10 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . purge_tasks ()","title":"purge_tasks"},{"location":"worker/methods/purge_tasks/#worker-purge_tasks","text":"The purge_tasks method deleted all the tasks from the queue. It allows the worker to implement some critical functionality where it identified a situation that should stop all the future tasks from being executed. Unless task_name was specified, uses the current worker name.","title":"Worker - purge_tasks"},{"location":"worker/methods/purge_tasks/#definition","text":"1 2 3 4 def purge_tasks ( self , task_name : typing . Optional [ str ] = None , ) -> bool","title":"Definition"},{"location":"worker/methods/purge_tasks/#examples","text":"1 2 3 4 5 6 7 8 9 10 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . purge_tasks ()","title":"Examples"},{"location":"worker/methods/push_task/","text":"Worker - push_task # The push_task method pushes a task onto the queue. Unless task_name was specified, uses the current worker name. kwargs - A dictionary of serializable arguments to pass to the worker. task_name - The name of the task/queue to push to. priority : NORMAL - The task will be pushed on the top of the queue. Will be pulled last. [FIFO] HIGH - The task will be pushed to the bottom of the queue. Will be pulled first. [LIFO] consumable_from - Timestamp of when the task should be considered as a consumable task and can be popped from the queue. Definition # 1 2 3 4 5 6 7 def push_task ( self , kwargs : typing . Dict [ str , typing . Any ], task_name : typing . Optional [ str ] = None , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> bool Examples # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . retry ( task = task , consumable_from = int ( time . time () + 60 * 30 ), ) else : self . push_task ( kwargs = { 'html' : response . content , }, task_name = 'parse_html' , priority = 'NORMAL' , )","title":"push_task"},{"location":"worker/methods/push_task/#worker-push_task","text":"The push_task method pushes a task onto the queue. Unless task_name was specified, uses the current worker name. kwargs - A dictionary of serializable arguments to pass to the worker. task_name - The name of the task/queue to push to. priority : NORMAL - The task will be pushed on the top of the queue. Will be pulled last. [FIFO] HIGH - The task will be pushed to the bottom of the queue. Will be pulled first. [LIFO] consumable_from - Timestamp of when the task should be considered as a consumable task and can be popped from the queue.","title":"Worker - push_task"},{"location":"worker/methods/push_task/#definition","text":"1 2 3 4 5 6 7 def push_task ( self , kwargs : typing . Dict [ str , typing . Any ], task_name : typing . Optional [ str ] = None , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> bool","title":"Definition"},{"location":"worker/methods/push_task/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . retry ( task = task , consumable_from = int ( time . time () + 60 * 30 ), ) else : self . push_task ( kwargs = { 'html' : response . content , }, task_name = 'parse_html' , priority = 'NORMAL' , )","title":"Examples"},{"location":"worker/methods/push_tasks/","text":"Worker - push_tasks # The push_tasks method pushes multiple tasks onto the queue in a bulk insert. Unless task_name was specified, uses the current worker name. This method is similar to push_task except it gets a list of kwargs and pushes much much faster. kwargs_list - A list of dictionaries of serializable arguments to pass to the worker. task_name - The name of the task/queue to push to. priority : NORMAL - The tasks will be pushed on the top of the queue. Will be pulled last. [FIFO] HIGH - The tasks will be pushed to the bottom of the queue. Will be pulled first. [LIFO] consumable_from - Timestamp of when the task should be considered as a consumable task and can be popped from the queue. Definition # 1 2 3 4 5 6 7 def push_tasks ( self , kwargs_list : typing . Iterable [ typing . Dict [ str , typing . Any ]], task_name : typing . Optional [ str ] = None , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> bool Examples # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . retry ( task = task , consumable_from = int ( time . time () + 60 * 30 ), ) urls = self . extract_urls ( response . content ) self . push_tasks ( kwargs_list = [ { 'url' : url , } for url in urls ], task_name = 'crawl_url' , priority = 'NORMAL' , )","title":"push_tasks"},{"location":"worker/methods/push_tasks/#worker-push_tasks","text":"The push_tasks method pushes multiple tasks onto the queue in a bulk insert. Unless task_name was specified, uses the current worker name. This method is similar to push_task except it gets a list of kwargs and pushes much much faster. kwargs_list - A list of dictionaries of serializable arguments to pass to the worker. task_name - The name of the task/queue to push to. priority : NORMAL - The tasks will be pushed on the top of the queue. Will be pulled last. [FIFO] HIGH - The tasks will be pushed to the bottom of the queue. Will be pulled first. [LIFO] consumable_from - Timestamp of when the task should be considered as a consumable task and can be popped from the queue.","title":"Worker - push_tasks"},{"location":"worker/methods/push_tasks/#definition","text":"1 2 3 4 5 6 7 def push_tasks ( self , kwargs_list : typing . Iterable [ typing . Dict [ str , typing . Any ]], task_name : typing . Optional [ str ] = None , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> bool","title":"Definition"},{"location":"worker/methods/push_tasks/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) blocked = self . are_we_blocked ( response ) if blocked : self . retry ( task = task , consumable_from = int ( time . time () + 60 * 30 ), ) urls = self . extract_urls ( response . content ) self . push_tasks ( kwargs_list = [ { 'url' : url , } for url in urls ], task_name = 'crawl_url' , priority = 'NORMAL' , )","title":"Examples"},{"location":"worker/methods/requeue/","text":"Worker - requeue # The requeue method pushes back the task to the queue without increasing the run count by one. The way it works is by raising a WorkerRequeue exception which cause the worker to interrupt. It means you should call requeue only if you have nothing more to do. Calling requeue and catching the exception will not interrupt the worker but the task would be pushed back to the queue anyway. requeue makes the task to be considered as a failed task. consumable_from is a timestamp of when the task should be considered as a consumable task and can be popped from the queue. Requeue from handlers One should never call requeue from the following handlers: on_success on_retry on_max_retries on_requeue Definition # 1 2 3 4 5 6 def requeue ( self , task : sergeant . objects . Task , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> None Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . requeue ( task = task , ) OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) response . raise_for_status () def on_failure ( self , task , exception , ): self . requeue ( task = task , )","title":"requeue"},{"location":"worker/methods/requeue/#worker-requeue","text":"The requeue method pushes back the task to the queue without increasing the run count by one. The way it works is by raising a WorkerRequeue exception which cause the worker to interrupt. It means you should call requeue only if you have nothing more to do. Calling requeue and catching the exception will not interrupt the worker but the task would be pushed back to the queue anyway. requeue makes the task to be considered as a failed task. consumable_from is a timestamp of when the task should be considered as a consumable task and can be popped from the queue. Requeue from handlers One should never call requeue from the following handlers: on_success on_retry on_max_retries on_requeue","title":"Worker - requeue"},{"location":"worker/methods/requeue/#definition","text":"1 2 3 4 5 6 def requeue ( self , task : sergeant . objects . Task , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> None","title":"Definition"},{"location":"worker/methods/requeue/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . requeue ( task = task , ) OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) response . raise_for_status () def on_failure ( self , task , exception , ): self . requeue ( task = task , )","title":"Examples"},{"location":"worker/methods/respawn/","text":"Worker - respawn # The respawn method is used to respawn the worker's process. The supervisor gets the exception, and triggers a new worker process instead. This method can be used to intentionally kill the current worker process and spawn another one. Reasons for using this method can be detection of some bad worker state that can be fixed by restarting the process. Definition # 1 2 3 def respawn ( self , ) -> None Examples # Simple 1 2 3 4 5 6 7 8 9 10 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if self . memory_usage () > self . max_memory_usage : self . respawn ()","title":"respawn"},{"location":"worker/methods/respawn/#worker-respawn","text":"The respawn method is used to respawn the worker's process. The supervisor gets the exception, and triggers a new worker process instead. This method can be used to intentionally kill the current worker process and spawn another one. Reasons for using this method can be detection of some bad worker state that can be fixed by restarting the process.","title":"Worker - respawn"},{"location":"worker/methods/respawn/#definition","text":"1 2 3 def respawn ( self , ) -> None","title":"Definition"},{"location":"worker/methods/respawn/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if self . memory_usage () > self . max_memory_usage : self . respawn ()","title":"Examples"},{"location":"worker/methods/retry/","text":"Worker - retry # The retry method pushes back the task to the queue while increasing the run count by one. The way it works is by raising a WorkerRetry exception which cause the worker to interrupt. It means you should call retry only if you have nothing more to do. Calling retry and catching the exception will not interrupt the worker but the task would be pushed back to the queue anyway. retry makes the task to be considered as a failed task. consumable_from is a timestamp of when the task should be considered as a consumable task and can be popped from the queue. Using consumable_from parameter allows you to retry in a different time in case of temporary failure that will be resolved later. Retry from handlers One should never call retry from the following handlers: on_success on_retry on_max_retries on_requeue Definition # 1 2 3 4 5 6 def retry ( self , task : sergeant . objects . Task , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> None Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . retry ( task = task , ) OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) response . raise_for_status () def on_failure ( self , task , exception , ): self . retry ( task = task , )","title":"retry"},{"location":"worker/methods/retry/#worker-retry","text":"The retry method pushes back the task to the queue while increasing the run count by one. The way it works is by raising a WorkerRetry exception which cause the worker to interrupt. It means you should call retry only if you have nothing more to do. Calling retry and catching the exception will not interrupt the worker but the task would be pushed back to the queue anyway. retry makes the task to be considered as a failed task. consumable_from is a timestamp of when the task should be considered as a consumable task and can be popped from the queue. Using consumable_from parameter allows you to retry in a different time in case of temporary failure that will be resolved later. Retry from handlers One should never call retry from the following handlers: on_success on_retry on_max_retries on_requeue","title":"Worker - retry"},{"location":"worker/methods/retry/#definition","text":"1 2 3 4 5 6 def retry ( self , task : sergeant . objects . Task , priority : str = 'NORMAL' , consumable_from : int = 0 , ) -> None","title":"Definition"},{"location":"worker/methods/retry/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . retry ( task = task , ) OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) response . raise_for_status () def on_failure ( self , task , exception , ): self . retry ( task = task , )","title":"Examples"},{"location":"worker/methods/stop/","text":"Worker - stop # The stop method is used to stop the worker from keep running. The request is propogated to the supervisor, and the supervisor will never spawn a new worker instead. This behavior exists for situations where the worker encountered a problem without a solution. A proper use for this method is for example to use in combination with on_starvation. When a worker is starving, and there are not enough tasks to consume, the worker can be stopped intentionally to reduce the load from the queue. Also, if there is an external autoscaler in place, it can delete the instance because it has no more worker running. Definition # 1 2 3 def stop ( self , ) -> None Examples # Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) try : self . database . insert_crawl_result ( url = url_to_crawl , content = response . content , ) except pymongo . errors . ServerSelectionTimeoutError : self . stop () OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) self . database . insert_crawl_result ( url = url_to_crawl , content = response . content , ) def on_failure ( self , task , exception , ): if isinstance ( exception , pymongo . errors . ServerSelectionTimeoutError ): self . stop ()","title":"stop"},{"location":"worker/methods/stop/#worker-stop","text":"The stop method is used to stop the worker from keep running. The request is propogated to the supervisor, and the supervisor will never spawn a new worker instead. This behavior exists for situations where the worker encountered a problem without a solution. A proper use for this method is for example to use in combination with on_starvation. When a worker is starving, and there are not enough tasks to consume, the worker can be stopped intentionally to reduce the load from the queue. Also, if there is an external autoscaler in place, it can delete the instance because it has no more worker running.","title":"Worker - stop"},{"location":"worker/methods/stop/#definition","text":"1 2 3 def stop ( self , ) -> None","title":"Definition"},{"location":"worker/methods/stop/#examples","text":"Simple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) try : self . database . insert_crawl_result ( url = url_to_crawl , content = response . content , ) except pymongo . errors . ServerSelectionTimeoutError : self . stop () OnFailure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) self . database . insert_crawl_result ( url = url_to_crawl , content = response . content , ) def on_failure ( self , task , exception , ): if isinstance ( exception , pymongo . errors . ServerSelectionTimeoutError ): self . stop ()","title":"Examples"},{"location":"worker/worker/finalize/","text":"Worker - finalize # The finalize method is invoked by the once after it has exceeded the maximum number of tasks per run. This is the opportunity to collect metrics, to close handles, and to perform cleanups. Definition # 1 2 3 def finalize ( self , ) -> None Examples # 1 2 3 4 5 def finalize ( self , ): self . apm_client . close () self . mongo . close ()","title":"finalize"},{"location":"worker/worker/finalize/#worker-finalize","text":"The finalize method is invoked by the once after it has exceeded the maximum number of tasks per run. This is the opportunity to collect metrics, to close handles, and to perform cleanups.","title":"Worker - finalize"},{"location":"worker/worker/finalize/#definition","text":"1 2 3 def finalize ( self , ) -> None","title":"Definition"},{"location":"worker/worker/finalize/#examples","text":"1 2 3 4 5 def finalize ( self , ): self . apm_client . close () self . mongo . close ()","title":"Examples"},{"location":"worker/worker/initialize/","text":"Worker - initialize # The initialize method is invoked by the worker once at the moment the worker is spawned by the supervisor . This method allows to implement an initialization of object that will live for the whole lifespan of the worker. A good usage example is to initialize a Logger object or an APM object. Definition # 1 2 3 def initialize ( self , ) -> None Examples # 1 2 3 4 5 def initialize ( self , ): self . my_logger = logging . getLogger () self . apm_client = elasticapm . Client ()","title":"initialize"},{"location":"worker/worker/initialize/#worker-initialize","text":"The initialize method is invoked by the worker once at the moment the worker is spawned by the supervisor . This method allows to implement an initialization of object that will live for the whole lifespan of the worker. A good usage example is to initialize a Logger object or an APM object.","title":"Worker - initialize"},{"location":"worker/worker/initialize/#definition","text":"1 2 3 def initialize ( self , ) -> None","title":"Definition"},{"location":"worker/worker/initialize/#examples","text":"1 2 3 4 5 def initialize ( self , ): self . my_logger = logging . getLogger () self . apm_client = elasticapm . Client ()","title":"Examples"},{"location":"worker/worker/post_work/","text":"Worker - post_work # The post_work method is invoked by the worker for every execution of a task, after it has finished a work run. This method allows the user to perform an operation after every task is executed. It can be used to close an APM transaction, or to send a metrics. Definition # 1 2 3 4 5 6 def post_work ( self , task : sergeant . objects . Task , success : bool , exception : typing . Optional [ BaseException ], ) -> None Examples # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def post_work ( self , task , success , exception , ): self . my_logger . debug ( f 'stopped working on { task . kwargs [ \"url\" ] } : { time . time () } , exception: { exception } ' ) if exception is not None : self . apm_client . capture_exception () self . apm_client . end_transaction ( result = 'success' if success else 'failure' , )","title":"post_work"},{"location":"worker/worker/post_work/#worker-post_work","text":"The post_work method is invoked by the worker for every execution of a task, after it has finished a work run. This method allows the user to perform an operation after every task is executed. It can be used to close an APM transaction, or to send a metrics.","title":"Worker - post_work"},{"location":"worker/worker/post_work/#definition","text":"1 2 3 4 5 6 def post_work ( self , task : sergeant . objects . Task , success : bool , exception : typing . Optional [ BaseException ], ) -> None","title":"Definition"},{"location":"worker/worker/post_work/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 def post_work ( self , task , success , exception , ): self . my_logger . debug ( f 'stopped working on { task . kwargs [ \"url\" ] } : { time . time () } , exception: { exception } ' ) if exception is not None : self . apm_client . capture_exception () self . apm_client . end_transaction ( result = 'success' if success else 'failure' , )","title":"Examples"},{"location":"worker/worker/pre_work/","text":"Worker - pre_work # The pre_work method is invoked by the worker for every execution of a task, prior to the work method. This method allows the user to perform an operation before every task is executed. It can be used to open an APM transaction, to measure the time before a task and more. The timeouts parameter would not affect the execution of this method. The killer has no effect on this method. It means that if the user will run something infinitely here, it will stuck the worker forever. Definition # 1 2 3 4 def pre_work ( self , task : sergeant . objects . Task , ) -> None Examples # 1 2 3 4 5 6 def pre_work ( self , task , ): self . my_logger . debug ( f 'started working on { task . kwargs [ \"url\" ] } : { time . time () } ' ) self . apm_client . begin_transaction ()","title":"pre_work"},{"location":"worker/worker/pre_work/#worker-pre_work","text":"The pre_work method is invoked by the worker for every execution of a task, prior to the work method. This method allows the user to perform an operation before every task is executed. It can be used to open an APM transaction, to measure the time before a task and more. The timeouts parameter would not affect the execution of this method. The killer has no effect on this method. It means that if the user will run something infinitely here, it will stuck the worker forever.","title":"Worker - pre_work"},{"location":"worker/worker/pre_work/#definition","text":"1 2 3 4 def pre_work ( self , task : sergeant . objects . Task , ) -> None","title":"Definition"},{"location":"worker/worker/pre_work/#examples","text":"1 2 3 4 5 6 def pre_work ( self , task , ): self . my_logger . debug ( f 'started working on { task . kwargs [ \"url\" ] } : { time . time () } ' ) self . apm_client . begin_transaction ()","title":"Examples"},{"location":"worker/worker/work/","text":"Worker - work # The work method is the method that should include our work login. This is where the task should be executed. The task input parameter should include all the information for the worker to perform its logic. Many worker methods pass and get the task object so they can function properly. 1 2 3 4 5 6 7 8 9 @dataclasses . dataclass class Task : kwargs : typing . Dict [ str , typing . Any ] = dataclasses . field ( default_factory = dict , ) date : int = dataclasses . field ( default_factory = lambda : int ( time . time ()), ) run_count : int = 0 date - This is the date the task object was created and pushed to the queue. kwargs - This is a dictionary of arguments that were passed to the worker. run_count - This is the number of times the task was executed. Definition # 1 2 3 4 def work ( self , task : sergeant . objects . Task , ) -> typing . Any Examples # 1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . retry () self . mongo . crawling_db . webpages . insert_one ( ... )","title":"work"},{"location":"worker/worker/work/#worker-work","text":"The work method is the method that should include our work login. This is where the task should be executed. The task input parameter should include all the information for the worker to perform its logic. Many worker methods pass and get the task object so they can function properly. 1 2 3 4 5 6 7 8 9 @dataclasses . dataclass class Task : kwargs : typing . Dict [ str , typing . Any ] = dataclasses . field ( default_factory = dict , ) date : int = dataclasses . field ( default_factory = lambda : int ( time . time ()), ) run_count : int = 0 date - This is the date the task object was created and pushed to the queue. kwargs - This is a dictionary of arguments that were passed to the worker. run_count - This is the number of times the task was executed.","title":"Worker - work"},{"location":"worker/worker/work/#definition","text":"1 2 3 4 def work ( self , task : sergeant . objects . Task , ) -> typing . Any","title":"Definition"},{"location":"worker/worker/work/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 def work ( self , task , ): url_to_crawl = task . kwargs [ 'url' ] response = requests . get ( url_to_crawl ) if not response . ok : self . retry () self . mongo . crawling_db . webpages . insert_one ( ... )","title":"Examples"}]}